

# 模型评估与选择



## 归纳偏好：

> 判断为正例的假设有多种，我们的算法更喜欢哪一种假设将其判断为正例
>
> 定义：机器学习算法在学习过程中对某种类型假设的偏好，称为归纳偏好。
>
> *  任何机器学习算法必定有其归纳偏好，否则学得模型后，它时而告诉我们是好的，时而又是坏的，没有意义
>
> 在回归问题中，有很多条曲线拟合，我们的算法必须有某种偏好，才能产生出它认为正确的模型



奥卡姆剃刀：若有多个假设与观察一致，则选择最简单的那个



## 没有免费的午餐定理：

> 无论学习的算法A有多么聪明，算法B有多么笨拙，它们的总误差相同！！！
>
> 前提：所有问题出现的机会相同或所有问题同等重要

**脱离具体的问题，空泛的谈什么算法更好，毫无意义**



## 经验误差和过拟合

错误率：分类错误的样本占样本总数的比例

精度 = 1- 错误率

学习器预测输出与实际输出之间的差异称为 “误差”

训练误差/经验误差 ： 学习器在训练集上的误差

泛化误差：在新样本上的误差



我们希望在训练样本中让学习器学习到潜在样本的普遍规律，这样在遇到新样本时才能做出正确的判别

过拟合：

> 学习器把训练样本学习的太好了，很可能把训练样本自身上的某些特点当成了所有潜在样本的特征了，这样会导致泛化性能的下降

欠拟合：

> 相反，欠拟合就是训练集上的一般性质还没有学好



## 评估方法

### 留出法

> 将数据集划分出两个，一个训练集一个测试集。
>
> 采样方式为“分层采样”，具体过程：70%训练集，30%测试集，训练集和测试集中正例和反例的比例都相等
>
> * 单次使用留出法得到的结果不可靠稳定，一般采用若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果
> * 训练集/测试集的划分要保持数据分布的一致性，避免因数据划分过程而引入额外的偏差而对最终结果产生影响



```

```



### 交叉验证

> 将数据集划分成k个大小相似的互斥子集，每个子集都要保持数据分布的一致性（从D中通过分层抽样得到的），k-1个子集做训练集，最后一个作为测试集，每一次划分出一个，划分十次取最后的平均值

```
clf = svm.SVC(kernel='linear', C=1)
scores = cross_val_score(clf, iris.data, iris.target, cv=5)  #cv为迭代次数。
print(scores)  # 打印输出每次迭代的度量值（准确度）
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))  # 获取置信区间。（也就是均值和方差）

# ===================================多种度量结果======================================
scoring = ['precision_macro', 'recall_macro'] # precision_macro为精度，recall_macro为召回率
scores = cross_validate(clf, iris.data, iris.target, scoring=scoring,cv=5, return_train_score=True)
sorted(scores.keys())
print('测试结果：',scores)  # scores类型为字典。包含训练得分，拟合次数， score-times （得分次数）
```



```python
# 分层K折交叉验证、分层随机交叉验证
skf = StratifiedKFold(n_splits=3)  #各个类别的比例大致和完整数据集中相同
for train, test in skf.split(iris.data, iris.target):
    print("分层K折划分：%s %s" % (train.shape, test.shape))
    break

skf = StratifiedShuffleSplit(n_splits=3)  # 划分中每个类的比例和完整数据集中的相同
for train, test in skf.split(iris.data, iris.target):
    print("分层随机划分：%s %s" % (train.shape, test.shape))
    break


#分组k-fold交叉验证、留一组交叉验证、留 P 组交叉验证、Group ShuffleSplit

X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]
y = ["a", "b", "b", "b", "c", "c", "c", "d", "d", "d"]
groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]

# k折分组
gkf = GroupKFold(n_splits=3)  # 训练集和测试集属于不同的组
for train, test in gkf.split(X, y, groups=groups):
    print("组 k-fold分割：%s %s" % (train, test))

# 留一分组
logo = LeaveOneGroupOut()
for train, test in logo.split(X, y, groups=groups):
    print("留一组分割：%s %s" % (train, test))

# 留p分组
lpgo = LeavePGroupsOut(n_groups=2)
for train, test in lpgo.split(X, y, groups=groups):
    print("留 P 组分割：%s %s" % (train, test))

# 随机分组
gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)
for train, test in gss.split(X, y, groups=groups):
    print("随机分割：%s %s" % (train, test))

```



### 划分子集的方法



留一法：

> 有k个样本，划分k个子集，用其中1个子集进行验证，k-1次验证，留一法在实际中的评估结果比较准确，但是计算量很大。

```
# 留一划分子集
loo = LeaveOneOut()
for train, test in loo.split(iris.data):
    print("留一划分：%s %s" % (train.shape, test.shape))
    break

# 留一分组
logo = LeaveOneGroupOut()
for train, test in logo.split(X, y, groups=groups):
    print("留一组分割：%s %s" % (train, test))
```

自助法：

> 每次随机从数据集D中挑选出一个样本，将其拷贝放入D‘ ，然后再把该样本放回去（该样本下一次抽取时还能被抽取到），这个过程执行m次
>
> * 有一部分数据会在D’ 中重复出现
> * 有一部分数据自始至终都不会抽取到 ，大约为 0.368
>
> 没有被抽取到的36.8%的数据可用作测试集，这样的测试结果称为“包外估计”
>
> * 自助法数据集较小，难以有效划分训练集/测试集时很有用
> * 自助法能产生多个不同的数据集，这对集成学习很有帮助



```
# k折划分子集
kf = KFold(n_splits=2)
for train, test in kf.split(iris.data):
    print("k折划分：%s %s" % (train.shape, test.shape))
    break

# 留一划分子集
loo = LeaveOneOut()
for train, test in loo.split(iris.data):
    print("留一划分：%s %s" % (train.shape, test.shape))
    break

# 留p划分子集
lpo = LeavePOut(p=2)
for train, test in loo.split(iris.data):
    print("留p划分：%s %s" % (train.shape, test.shape))
    break

# 随机排列划分子集
ss = ShuffleSplit(n_splits=3, test_size=0.25,random_state=0)
for train_index, test_index in ss.split(iris.data):
    print("随机排列划分：%s %s" % (train.shape, test.shape))
    break
```



## 性能度量

> 对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要衡量模型泛化能力的评价标准
>
> 评估学习器的性能，即预测结果与真实标记进行比较
>
> 回归任务最常用的就是均方误差

查准率：

> 在预测结果都是真的里面，有多少个真的是预测对的



查全率：

> 在真实结果都是真的里面，有多少个预测是真的

![Screenshot_20220227_155415](D:%5CHuawei%20Share%5CHuawei%20Share%5CScreenshot_20220227_155415.jpg)



P-R曲线：

> 查准率-查全率曲线
>
> 尽可能的让查准率和查全率都高，找到平衡点（查准率=查全率）

F1度量：

![Screenshot_20220227_161417](D:%5CHuawei%20Share%5CHuawei%20Share%5CScreenshot_20220227_161417.jpg)



#### ROC曲线

> 真正率：所有预测结果中正例是对的，假正率：所有预测结果中反例为对的
>
> 真正率为纵轴，假正率为横轴
>
> 我们根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算出两个重要的值（x,y），分别以它们为横轴、纵轴作图，得到ROC曲线

AUC：(Area Under ROC Curve)

> 比较ROC曲线下的面积，就是预测正确的
>
> 损失：ROC曲线上面的面积
>
> AUC曲线的意义：预测是对的大于预测是错的概率，即真正例率大于假正例率的所有点的集合--->面积



#### 方差：

​	预测值与平均值的差值的平方的期望，度量同样大小的数据集的变动导致的学习性能的变化

噪声：

​	数据集中的值和真实的值的差的平方的期望，表达了当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。

偏差：

​	预测值与真实值的差值的期望，度量学习器期望预测与真实结果的偏离程度

泛化误差 = 偏差+方差+噪声



####  Friedman费德曼检验：

> 交叉t检验和McNemar检验都是在一个数据集上比较两个算法的性能，更多的时候，我们是在一组数据集上检验多个算法。
>
> 1.在每个数据集上两两比较检验的结果
>
> 2.使用基于算法排序的Friedman检验

流程：

使用D1-D4四个数据集对算法A\B\C进行检验，在每个数据集上根据测试性能对算法由好到坏进行排序，

从好到坏：1，2，3，4   性能相同赋一样的值

最后得出4个数据集上平均的排名，进行排序

### 偏差-误差窘境

1.欠拟合状况下

> 即训练不足，学习器的拟合能力不够强，训练数据的扰动不足以使学习器发生显著变化
>
> 此时 偏差主导了泛化错误率

2.正常情况下

> 随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学习到
>
> 此时 方差主导错误率

3.过拟合情况下

> 训练程度足够强，训练数据集的轻微扰动都导致学习器发生显著变化
>
> 此时 泛化误差的来源为 ：训练集自身的特性，非全局的特性被学习器学习到了



## 线性判别分析：LDA

> 思想：
>
> 给定训练样例集，设法将样例投影到一条直线上，使得同类的样例的投影点尽可能的近，异类样例的点尽可能的远。在对新鲜样本进行分类的时候，将其投影到同样的直线上，再根据投影点的位置来判断新的样本的类别。

## 多分类学习

核心：利用二分类的思想来解决多分类的问题

> 有N个类别，我们就拆成N个2分类问题，然后为拆出的每个二分类任务训练一个分类器，在测试时，对这些分类器的预测结果进行集成以获得最终的分类结果。
>
> 关键：拆分与合成
>
> 拆分策略：
>
> 一对一（OVO）：
>
> ---
>
> 将这N个类，两两配对，得到N(N-1)/2个二分类任务，测试时，将新样本同时交给所有的分类器，于是得到N(N-1)/2个分类结果，最终结果可通过投票产生，把预测结果最多的类别作为最终结果。
>
> ---
>
> 一对多（OVR）：
>
> 每次将一个类的样例作为正例，所有的其它类的样例作为反例来训练N个分类器。
>
> **只要有一个分类器预测的结果是正例，那么最终结果就是这个分类器正例对应的标签**，其他分类器预测的不是正例就可以排除那个正例的分类器，因为负例分类器有多个，预测结果是负例，说明正确的结果位于负例中的某一类标签。
>
> 多对多（MVM）：
>
> 每次将若干个类作为正类，若干个其他类作为反类，OVM是MVM的特例。
>
> * MVM的正反类构造必须有特殊的设计  ---> 纠错输出码ECOC
>
> 编码：
>
> 对N个类做M次划分，每次划分将一部分类别划分为正类，一部分划分为负类，从而形成一个二分类的训练集；这样一共产生M个训练集，M个分类器。
>
> 解码：
>
> M个分类器分别对测试样本进行预测，这些预测标记组成一个编码（一列）。将这个编码与每一个类各自的编码进行比较（正的多，还是负的多），多的就是单个分类器最终的结果
>
> 编码矩阵：
>
> 二元码：将每个类别分为正例（1）和反例（-1）
>
> 三元码：除了正例和反例，还有一种停用类（0）
>
> 所有分类对应的编码集合起来的结果（每一列投票得到一个结果（纵向），然和合并（横向）），得到测试示例的编码，
>
> 再与每一个类的预测结果进行对比（横向看），算出距离最小的（纵向计算）那个类别就是最终的结果

为什么叫做纠错输出码？

这是因为在测试集上，ECOC有容忍和修正的能力。其中某一个分类器出错，导致测试示例的编码有错误，但是基于这个编码得出来的最后分类结果仍然是对的，这就是分类容忍和修正。

### 类别不平衡问题

> 如果数据集里面不同类别的训练样例数目差别很大，会对学习过程产生困扰。
>
> 比如：一个数据集里面有998个反例，只有2个正例，得出来的分类器预测的结果几乎都是反例，根本没有学习到正例的特性
>
> 即使，样本中正例和负例差不多，但是OVO和OVM之后，可能又有类别不平衡问题了。

从线性分类器的角度来理解：

> 预测结果与阈值进行比较，比如大于0.5为正，小于0.5为负
>
> y预测为正的概率 ，m+样本中正例的数目
>
> 正例：y/1-y > m+/m-    
>
> 负例：y/1-y <m+/m-

解决方法：再缩放

> (y/1-y) * (m+/m-)  进行缩放
>
> * 前提是训练集是样本总体的无偏采样 ， 但是这个假设往往不成立
>
> 1. 直接对样本中的反例进行欠采样，即去除一些反例，使得正例和反例的数目差不多
> 2. 对正例样本进行过采样，即增加一些正例，使得正例和反例的结果相当
> 3. 直接对原始数据集进行学习，但在用训练好的分类器进行预测，将再缩放的式子嵌入到去决策过程，称之为”阈值移动“

* 欠采样的时间开销小于过采样，因为前者丢弃了很多反例，使得训练集的数目小于测试集
* 过采样增加了很多正例，训练集大于原始数据集
* 过采样不能简单的对初始正例样本进行重复采样，否则会产生严重的过拟合，SMOTE对训练集中的正例进行插值来产生来产生额外的正例。
* 欠采样随机丢弃反例，会导致丢弃重要的信息，EasyEnsemble算法利用集成学习机制，将反例划分为若干个集合供不同的学习器进行学习，这样对每个学习器来看，它们都欠采样了，而对全局来说就没有过采样。







### 决策树



















#  KNN算法：

K近邻算法采用不同的特征值之间的距离方法进行分类

优点：精度高、对异常值不敏感、无数据假定

缺点：计算复杂度高、空间复杂度高

##### 工作原理：

存在一个样本集合，并且样本集合中每个标签都存在标签，即我们知道样本中每一个数据的标签与所属分类关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后提取样本集中特征最相似的数据（最近邻的标签）

#### KNN-sklearn使用：

```python
from sklearn.neighbors import NearestNeighbors

nbrs = NearstNeighbors(
    n_neighbors = 2 , algorithm = 'ball_tree/kd_tree/brute/auto').fit(X)


```



#### KDtree、Balltree

kd-tree：二叉树结构的树，通过有效编码样本的aggerate distance（聚合信息）来减少距离计算量，不需要明确				的计算它们的距离，**但是维度超过20，效率变低，维度灾难！**

ball-tree: 解决了kd-tree在高维度上的效率低下的问题，kd-tree沿笛卡尔坐标系分隔数据，ball-tree沿着超平面来				 分割数据，通过质心C和半径R定义的节点，使得每一个结点位于由R和C定义的超平面内，使用三角不等				 式减少近邻搜索点   |x+y|<=|x|+|y|  , balltre的结点时几何球形的

​				

| 参数列表： |                                                              |
| :--------- | ------------------------------------------------------------ |
| X          | (样本数，特征数即空间的维度)                                 |
| leaf_size  | 叶子结点的大小，默认40                                       |
| metric     | 距离指标表对象，用p=2(欧几里得指标)，默认=“minkowski”        |
| 使用：     | tree = BallTree(X, leaf_size=?)                              |
| 1          | distance（邻近的距离）, index(邻居的下标) = tree.query(X[ : 1] , k=3) |
| 2          | tree.query_radius(X[:1],r=0.3,count_only = True/False是显示哪三个邻居) |

#### 分类：

| RadiusNeighborsClassifier                                 | KNeighborsClassifier                                         |
| --------------------------------------------------------- | ------------------------------------------------------------ |
| 基于每一个查询点的固定半径r内的邻居数量，r是指定半径float | 高度依赖数据，通常较大的k会抑制噪声，但是使得分类界限不明显  |
| weight关键字（权重分配）                                  | uniform(统一权重)、distance（查询点与距离成反比）,强调一下距离，这种情况下距离查询点更近的点比更远的点将产生更大的影响 |
|                                                           |                                                              |

| 参数列表：             | 分类器参数就这几个                                  |
| ---------------------- | --------------------------------------------------- |
| fit                    | （X,y）                                             |
| get_params             |                                                     |
| predict                | (分类器对象)，返回预测的类标签                      |
| predict_proba          | 同，返回预测的概率估计                              |
| radius_neighbors       | (分类器对象，r=?)                                   |
| radius_neighbors_graph |                                                     |
| score                  |                                                     |
| kneighbors             | (X)  返回两个参数：在距离为多少的地方，是第几个样本 |

#### 回归

KNeighbors

```python
sklearn.neighbors.KNeighbors(
	n_neighbors = 5 , 基于邻居数的回归
	weight='uniform' ,
	algorithm='auto' ,
	leaf_size=30 ,
	p=2 ,
	metric='minkowski', 
	n_jobs=None 默认为1
	)
```

RadiusNeighborsRegressor

```
sklearn.neighbors.RadiusNeighborsRegressor(
	radius=1.0 ,  基于固定半径的回归
	weight='uniform' ,
	algorithm='auto' ,
	leaf_size=30 ,
	p=2 ,
	metric='minkowski', 
	n_jobs=None 默认为1
	)# 决策树
```

#### 分类 DecisionTreeClassifier \回归DecisionTreeRegression:



```
sklearn.tree.DecisionTreeClassifier(
	criteria ='gini'/'entropy',默认基尼
	splitter = {'best','random'}用于选择每个节点的策略
	max_depth= 
	max_feature = {'auto','sqrt','log2'} , 最佳有效划分区
	random_state =
	class_weight = 
)
```

```python
StandardScaler : 数据归一化处理
#集合管道进行操作
pipe = Pipeline(step = [('scaler',Standardscaler()),('svc',SVC())])

Pipeline的作用：

    Pipeline可以将许多算法模型串联起来，可以用于把多个estamitors级联成一个estamitor,比如将特征提				取、归一化、分类组织在一起形成一个典型的机器学习问题工作流。
    Pipleline中最后一个之外的所有estimators都必须是变换器（transformers），最后一个estimator可以是任意类型（transformer，classifier，regresser）,如果最后一个estimator是个分类器，则整个pipeline就可以作为分类器使用，如果最后一个estimator是个聚类器，则整个pipeline就可以作为聚类器使用。

        主要带来两点好处：

        1.直接调用fit和predict方法来对pipeline中的所有算法模型进行训练和预测。

        2.可以结合grid search对参数进行选择.

2.串行化用法：

(1)通过steps参数，设定数据处理流程。格式为('key','value')，key是自己设定的名称，value是对应的处理类。最后通过list将这些step传入。前n-1个step中的类都必须有transform函数，最后一步可有可无，一般最后一步为模型。使用最简单的iris数据集来举例：

in[ ]:

from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
iris=load_iris()
pipe=Pipeline(steps=[('pca',PCA()),('svc',SVC())])
pipe.fit(iris.data,iris.target)
out[ ]:

```







