# 								爬虫

### 概念：

**网络爬虫**：就是模拟客户端发送请求，获取响应数据，一种按照一定的规则，自动地抓取万维网上的信息的程序或者脚本

**爬虫分类:**

1. 通用爬虫：抓取系统中重要的组成部分。抓取的是一整张页面数据
2. 聚焦爬虫：建立在通用爬虫的基础上，抓取的是页面中特定的局部内容。
3. 增量式爬虫：检测网站中数据更新的情况，只会抓取网站中最新更新的数据

**爬虫的矛与盾**：

1. 反爬机制：门户网站，通过制定相关的策略或技术手段，防止爬虫程序对网站数据的爬取
2. 反反爬：破解门户网站具备的反爬机制，获取数据

**爬虫和浏览器的区别：**

​		浏览器：

​		浏览器向百度服务器发送请求 --> http ://www.baidu.com  

​		百度服务器发送数据--> 获取响应数据，进行渲染 -->给用户看

​		爬虫：

​		向百度服务器发送请求--> http ://www.baidu.com

​		百度服务器发送数据--> 获取响应数据，并保存数据

#### URI和URL:

URI全称：Uniform Resource Identifier (统一资源标识符)

URL全程：Universe Resource Locator (统一资源定位符) , URL是URI的子集

举例说明： https：／／github .com/favicon.ico  这是GitHub网站的图标链接

访问协议https · 访问路径github.com ·  资源名称favicon.ico 

#### 超文本：

hypertext--超文本	网页就是超文本解析而成的，网页的源代码就是HTML代码

#### HTTP和HTTPS协议：

HTTP：Hyper Text  Transfer Protocol	(超文本传输协议)，用于从网络传输超文本数据到本地浏览器的传送协议，能保证高效而准确的传输超文本文档

HTTPS：Hyper Text Transfer Protocol over Secure Socket Layer (以安全为目标的HTTP通道)，就是HTTP的安全版

就是服务器与客户端进行数据交互的一种形式。

常用请求头信息：

1. User-Agent：请求载体的身份标识，可以使服务器识别操作系统的版本、浏览器等信息
2. Conection:请求完之后，是断开连接还是保持连接
3. Cookie: 为了辨别用户进行会话跟踪而储存在用户本地的数据，功能是 维持当前访问会话。就是用户登陆后，服务器会保持登录状态

常用响应头信息：

​	Content-Type：服务器响应回客户端的媒体类型信息，例如html、json、gif

加密方式：

1. 对称密钥加密：客户端对数据进行加密，锁和密钥一起发给服务器，服务器再用密钥打开锁，发送过程中被拦截密钥会被拿走
2. 非对称密钥加密：服务器制定好加密方式发送给客户端，客户端用该加密方式对数据进行加密，再发送给服务器用私钥进行解密。但是加密方式传输时可能被拦截篡改。
3. 证书密钥加密：先把公开密钥给证书认证机构，给公钥进行签名，公钥封装到证书里，发送给客户端。客户端相信认证机构，客户端通过公钥对报文进行加密发送，服务器接收后用私钥进行解密

#### 响应：

由服务端返回给客户端，分为三部分。响应状态码（Response Status Code）,响应头（Response Headers）,响应体（Response Body）

1.状态码：

```
200--成功
400--错误请求
403--禁止访问
404--找不到请求的网页
502--错误网关
503--服务器目前无法使用
504--网关超时
```

2.响应头：响应头，Server、Cookies等信息

3.响应体:请求网页的时候，返回的使HTML代码，请求图片的时候响应的使二进制数据



#### 网页的结构：

title标签使网页的标题	·	body是正文内容

div是网页的区块标签，id=“ ” 	id的内容在网页中是唯一的	class=“  ” 属性标记

HTML文档是树结构，节点树

![1632450058735](C:%5CUsers%5C%E9%93%B6%E6%99%97%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1632450058735.png)

### 为什么有时候解析不到数据？

1.beautifulsoup的标签不规范，无法识别定位

2.xpath的标签嵌套找不到，定位不到

3.js的渲染

使用HTTP请求库得到的源代码和浏览器中看到的数据不一样 ，是因为js会改变HTML的结点，向其添加内容，最后得到浏览器的页面

4.不要在Elements的选项卡中直接查看源码，那里的代码给渲染过了，要在network中查看

### 什么是会话（Session）和Cookies

> HTTP的特点之一：无状态 
>
> 指HTTP协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么
>
> * 意味着 如果后续需要处理前面的信息，必须重传

#### Session

在服务端----网站的服务器

* 用来保存用户的Session信息

#### Cookies

在客户端也就可以理解为浏览器端 

* 辨别用户的身份、进行Session跟踪而储存再用户本地终端上的数据

**浏览器在下次访问网页时会自动附带上cookies发送给服务器，服务器通过识别Cookies并鉴定出时哪个用户，然后再判断用户是否是登陆状态进而返回对应的响应**

1.会话cookies:

​	cookies放在浏览器内存里，浏览器关闭之后该cookies失效

2.持久cookies:

​	浏览器关闭，cookies不会消失，cookies会保存会话ID信息到硬盘上，再次打开浏览器，仍然能够找到原来的会话ID，依旧可以处于保持登录状态。

恰恰是这样，这需要服务器为会话机制设置一个失效时间，距离客户端上一次书院会话的时间超过失效时间，服务器就会删除会话以节省空间

### 代理：

代理-->代理服务器，proxy server	

功能：代理网络用户去取得网络信息

理解：代理服务器就是在本机和服务器之间的一个桥，本机向代理发请求，然后代理再向服务器发请求，回来的响应也是经历这样的过程，web服务器无法识别我们的真实IP，从而实现IP伪装

## urllib：

有4个模块

request:	HTTP请求模块

error:	异常处理模块，如果出现错误，我们可以捕获异常，然后进行重试或者其他操作，保证程序不会意外终止

parse: 工具模块，提供URL的处理方法

robotparse:	识别网站的robot.txt

### requests

requests是一个优雅而简单的Python Http请求库，作用是发送请求获取响应数据

使用3步骤：

1. 导入模块

2. 发送get请求，获取响应

3. 从响应中获取数据

   ```python
   import requests
   
   get(url= ,param= ,headers=)
   url:请求的路由
   param:要访问抓取的参数，用抓包工具用网页上抓取，有些固定的参数自己可以更改，注意数	  据类型是str
   headers:User-Agent，抓包工具抓一次就好了
   timeout:超时时间，过了这个时间服务器没响应，就返回失败
    
   post(url= ,headers= ,data= )	注意和get区分
   
   response = requests.get('http://www.baidu.com')
   response.status_code //返回请求状态
   response.encoding = 'utf-8'
   # print(response.text)
   #content :获取响应的二进制数据，图片、图标
   response.text = response.content.decode()  
   # 默认utf-8 ，如果是gbk ，decode（encoding = ‘gbk’）
   
   text(字符串)，content(二进制) ，json(对象)
   
   ```

```python
#session使用
headers={
    ...
}
proxies={
    ...
}
s = request.Session()
s.header.update(headers)
r = s.get(url,params,headers,timeout,proxies)
r = s.post(url,data)
```



#### 常用UA：

```python
User_Agent = [
    "Mozilla/5.0 (iPod; U; CPU iPhone OS 4_3_2 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8H7 Safari/6533.18.5",
    "Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_3_2 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8H7 Safari/6533.18.5",
    "MQQBrowser/25 (Linux; U; 2.3.3; zh-cn; HTC Desire S Build/GRI40;480*800)",
    "Mozilla/5.0 (Linux; U; Android 2.3.3; zh-cn; HTC_DesireS_S510e Build/GRI40) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Mozilla/5.0 (SymbianOS/9.3; U; Series60/3.2 NokiaE75-1 /110.48.125 Profile/MIDP-2.1 Configuration/CLDC-1.1 ) AppleWebKit/413 (KHTML, like Gecko) Safari/413",
    "Mozilla/5.0 (iPad; U; CPU OS 4_3_3 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Mobile/8J2",
    "Mozilla/5.0 (Windows NT 5.2) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.122 Safari/534.30",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.202 Safari/535.1",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/534.51.22 (KHTML, like Gecko) Version/5.1.1 Safari/534.51.22",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A5313e Safari/7534.48.3",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A5313e Safari/7534.48.3",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A5313e Safari/7534.48.3",
    "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.202 Safari/535.1",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0; SAMSUNG; OMNIA7)",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0; XBLWP7; ZuneWP7)",
    "Mozilla/5.0 (Windows NT 5.2) AppleWebKit/534.30 (KHTML, like Gecko) Chrome/12.0.742.122 Safari/534.30",
    "Mozilla/5.0 (Windows NT 5.1; rv:5.0) Gecko/20100101 Firefox/5.0",
    "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.2; Trident/4.0; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET4.0E; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET4.0C)",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET4.0E; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET4.0C)",
    "Mozilla/4.0 (compatible; MSIE 60; Windows NT 5.1; SV1; .NET CLR 2.0.50727)",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)",
    "Opera/9.80 (Windows NT 5.1; U; zh-cn) Presto/2.9.168 Version/11.50",
    "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)",
    "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022; .NET4.0E; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET4.0C)",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/533.21.1 (KHTML, like Gecko) Version/5.0.5 Safari/533.21.1",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; ) AppleWebKit/534.12 (KHTML, like Gecko) Maxthon/3.0 Safari/534.12",
    "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 2.0.50727; TheWorld)",
    "Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16",
    "Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14",
    "Mozilla/5.0 (Windows NT 6.0; rv:2.0) Gecko/20100101 Firefox/4.0 Opera 12.14",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0) Opera 12.14",
    "Opera/12.80 (Windows NT 5.1; U; en) Presto/2.10.289 Version/12.02",
    "Opera/9.80 (Windows NT 6.1; U; es-ES) Presto/2.9.181 Version/12.00",
    "Opera/9.80 (Windows NT 5.1; U; zh-sg) Presto/2.9.181 Version/12.00",
    "Opera/12.0(Windows NT 5.2;U;en)Presto/22.9.168 Version/12.00",
    "Opera/12.0(Windows NT 5.1;U;en)Presto/22.9.168 Version/12.00",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1",
    "Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10; rv:33.0) Gecko/20100101 Firefox/33.0",
    "Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/31.0",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20130401 Firefox/31.0",
    "Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20100101 Firefox/31.0",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:29.0) Gecko/20120101 Firefox/29.0",
    "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:25.0) Gecko/20100101 Firefox/29.0",
    "Mozilla/5.0 (X11; OpenBSD amd64; rv:28.0) Gecko/20100101 Firefox/28.0",
    "Mozilla/5.0 (X11; Linux x86_64; rv:28.0) Gecko/20100101  Firefox/28.0",
    "Mozilla/5.0 (Windows NT 6.1; rv:27.3) Gecko/20130101 Firefox/27.3",
    "Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:27.0) Gecko/20121011 Firefox/27.0",
    "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:25.0) Gecko/20100101 Firefox/25.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:25.0) Gecko/20100101 Firefox/25.0",
    "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:24.0) Gecko/20100101 Firefox/24.0",
    "Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:24.0) Gecko/20100101 Firefox/24.0"
]
```



#### UA：

UA= User-Agent(请求载体的身份标识)

UA伪装：门户网站的服务器会检测对应请求的载体身份标识。如果是某一款浏览器就是一个正常的请求，如果不是就是爬虫，是不正常的请求，服务端可能会拒绝请求，是拿不到数据的。

**为了避免被拒绝，爬虫需要进行UA伪装：让爬虫对应的身份标识伪装成某一个浏览器**

将 User-Agent封装到一个字典中:

headers={

​		’ User-Agent‘ = ‘ 网站上的 User-Agent对应的字符串’

}

```python
response.json()  //拿json数据
filename= input+'.json'
fp= open('./douban.json','w', encording='utf-8')
dump.(filename, fp= fp, ensure_ascii=False)
```

#### 代理：

破解封IP这种反扒机制，突破自身IP访问的限制，隐藏自身IP不会被封。	什么是代理服务器：作为一个中转服务器，我们的IP发给代理，代理再发IP给服务器，我们的IP地址不会别封。

```pyhon
代理相关网站：快代理、西祠代理、www.goubanjia.com

代理ip类型：
http：应用到http协议对应的url中
https：应用到https协议对应的url中


代理ip的透明度:
-透明：服务器知道该次请求使用了代理，也知道真实的ip
-匿名：知道使用了代理，不知道真实的ip
-高匿：不知道使用了代理也不知道真实的ip

例如：
proxy = '121.230.211.41'
如果要用户名： proxy = usernamme:password@'121.230.211.41'
proxies={'HTTP/HTTPS':'ip地址',
			  'HTTP':'http'+proxy}
page_text= requests.get(url=url,headers=headers,proxies=proxies).text
```



## 数据解析：

数据解析：Beautifulsoup，正则表达式，xpath

概念：解析的局部标签对应的属性中存储的数据值进行提取

1. 进行标签的定位
2. 标签或者标签对应的属性中储存的数据值进行提取

解析图片：

```
url= '...'
img_data= requests.get(url=url).content

with open('./giutu.jpg','wb') as fp:
	fp.write(img_data)
```

bs4数据解析：

1. 实例化一个BeautifulSoup对象，并将页面源码数据加载到该对象中
2. 通过BeautifulSoup对象的属性或者方法进行标签定位和数据提取

### Beautiful Soup

BeautifulSoup 对象：代表要解析整个文档树，它支持 遍历文档树 和 搜索文档树 中描述的大部分的方法

它的作用就是从 HTML 和 BeautifulSoup 对象的创建

```python
fp=open('.html','r',encording='utf-8')
soup= BeautifulSoup(fp,'lxml')

soup.tagName():返回的是html中第一次出现的tagName，tagName就是标签名
    
```

### **find（）方法：**

find的作用就是找标签

步骤：

1. 导入模块
2. 准备文档字符串
3. 创建BeautifulSoup标签
4. 查找文档中的标签

find（self ,name=, attrs={}, recursive=True, text= ,**kwargs ）

find_all(name , attrs , recursive , text,**kargs)



参数：

1. name: 标签名
2. attrs：属性字典
3. recursive： 是否递归查找
4. text: 根据文本内容查找

返回：查找到的第一个元素对象

```python
soup = BeautifulSoup('<html>data</html>', 'lxml')  
# BeautifulSoup 会自动修正代码print(soup)

#要准备html的文档
html='''
		...		'''
soup= BeautifulSoup(html , 'lxml')
print(soup)

#查找title和a标签
title= soup.find('title')
a= soup,find('a')  	
#属性定位
soup.find('div',class_=' song')  两个属性定位，标签+属性（可以是id或者attr）

#查找所有的a
soup= find_all('a')
print(a+title)

soup.select('.属性名')  . 代表class_   id/
soup.select('.tang >ul>li>a')[0] 	标签层级选择器，大于号表示一个层级,[]表示第几个
soup.select('.tang >ul a')[0]		空格一次性表示多个层级

soup.select('.tang >ul a')[0].text  直接获取文本

soup.a['标签名']  直接提取文本
#根据属性进行查找
	#查找id 为link1的标签
    a= soup.find(id='link1')
    a= soup.find(attrs={'id':'link1'})		#两种方法
    print(a)
#根据文本内容进行查找
text= soup.find(text=' 要查找的内容')

#Tag对象
print('标签名',a.name)
print('标签所有的属性',a.attrs)
print('标签文本内容',a.text)
Tag可以获取标签的属性和文本
```

### Xpath：

原理：实例化一个etree的对象，且需要将被解析的页面的源码数据加载到该对象中，调用xpath对象的xpath方法获取标签的定位和内容捕获

如何实例化etree对象：

1. 将本地的html文档的源码数据加载到etree中：etree.parse(filepath)
2. 可以将互联网上获取的源码数据加载到该对象中：etree.HTML(‘page_text’)
3. xpath(‘xpath的表达式’)

```python
tree = etree.parse('test.html')

r= tree.xpath('./html/body/div')
r=tree.xpath('./html/html/div')

#属性定位 tag[@attrname='attrValue']:
r=tree.xpath('//div[@class='song']')

#索引定位 /p[]索引从1开始：(//[@class=标签名]小标签名[第几个])
r=tree.xpath('//div[@class='song']/p[3]')	

#精确文本定位,[0]小技巧去掉列表符
#   /text():拿直系标签的文本
#	//text():拿所有的文本内容
r= tree.xpath('//div[@class='tang']//li[5]/a/text()')[0]	
r= tree.xpath('//div//li[7]//text')[0]

#取属性值  /@attrname
r=tree.xpath('//div[@class='song']/img/@scr')

#属性是多值的匹配 contains(@属性名，标签名)
<li class "li li-first">first item>
result = html.xpath('//li[contain(@class,"li")]/text()')


```

### PyQuery

* 使用的是CSS选择器

```python
from pyquery import PyQuery as pq
doc = pq(url="...")


doc('#container .list li')

#调用items方法后，会返回一个生成器
for item in doc('#container .list li').items():
    print(item.text())
```

* find()：``item.find('css选择器')``查找该节点的所有子孙节点
* children(): 只拿子节点
* parent() / parents()：获取节点的(单个/多个)父节点
* siblings(): 获取兄弟节点

**pyquery选择返回的结果都是pyquery类型的，可以进行类型转换**



#### 获取属性值：

![1635860179367](C:%5CUsers%5C%E9%93%B6%E6%99%97%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1635860179367.png)

或者：a.attr.href

#### 获取文本：

a.text()  返回的是该节点的所有的文本，字符串类型

a.html() 返回当前节点文本

#### 动态更改节点属性

1.addClass 和removeClass

* 动态更改class属性

![1635860890528](C:%5CUsers%5C%E9%93%B6%E6%99%97%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1635860890528.png)



2.attr方法对属性进行操作

![1635861027622](C:%5CUsers%5C%E9%93%B6%E6%99%97%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1635861027622.png)

3.text()和html()

* 可以更改节点内部的内容

4.append、empty、prepend方法







### Unicode乱码尝试方法：

UnicodeError是在处理字符串时出现的错误，分为两个子异常类，UnicodeEncode和UnicodeDecodeError

处理这个问题需要使用 encode() 和 decode()方法

```python
encode和decode均可接收encoding与errors两个参数，用来指定编码、解码的错误和出现错误的时候的反应
#注意！解码和解码前后的字符串的长度可能不同，因为二者的长度意义不同。对于Unicode长度为其中字符的长度，str的长度取决于bytes的数目
encode ：将unicode字符串翻译成bytes，也就是str对象
decode ：相反，将bytes翻译为原有的Unicode字符串

unicode字符串就是str->  'hello world'
bytes字符串->	b'hello world'
```

Encoding Error: UnicodeDecodeError

报错原因：由于指定的编码格式不足以编码指定unicode字符串中的某些字符，说明使用的时错误的编码格式

```python
>>>unicode_seq.encode('ascii')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
UnicodeEncodeError: 'ascii' codec can't encode character u'\xf8' in position 13: ordinal not in range(128)
#ascii无法编码u'\xf8'字符
```

Decoding Error:UnicodeDecodeError

报错原因：你指定的编码格式无法解码该字符串

```python
>>> wrong_seq.decode('utf-8')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/encodings/utf_8.py", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x89 in position 1: invalid start byte
#utf-8无法解码byte流
```

解决报错的思路：

```python
思路一：Unicode编码解码的顺序
程序的内部是Unicode字符串，程序外部是bytes字符串，所以在程序的入口解码读取到的数据，在程序的出口编码数据
```

 ![Unicode Sandwich](C:%5CUsers%5C%E9%93%B6%E6%99%97%5CPictures%5CSaved%20Pictures%5C70) 

```
思路二：了解被处理的数据是那种类型
两个方法：1.type()方法查看这个数据的类型  2.repr()方法查看它的Unicode字符串到底是什么
```



```
#乱码尝试方法1
 中文转Unicode编码：
 	text = '中国'
 	result = text.encode('unicode_escape')
 Unicode转中文：
 	result = u_str.decode('unicode_escape')

# 乱码尝试方法2
 1.图片：img_name = img_name.encode('iso-8859-1').decode('gbk')
 2.响应数据：response.encoding = 'utf-8'
```

![1632404127737](C:%5CUsers%5C%E9%93%B6%E6%99%97%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1632404127737.png)

### Excel：

#### 读取excel，保存为了excel：

```python
 df = pd.DataFrame()

    for i in data:
        df = df.append([[
        i['rank'],
        i['score'],
        i['types'],
        i['regions'],
        i["release_date"],
        i['title'],
        i['actors']]]
        )
    df.columns = ['排名','豆瓣评分','电影类型','国家','上映日期','电影名称','演员']
    df = df.reset_index(drop=True)
    # print(df)

#写入pandas的同时直接保存到excel
#列表符号那些，可以用excel中自带功能进行删除替换处理，快捷键Ctrl+H
    df.to_excel('D:/360Downloads/豆瓣喜剧TOP100.xlsx',sheet_name='Top100',index=False)
```

![img](https://img-blog.csdnimg.cn/20210209224318676.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FmeHRpYW4=,size_16,color_FFFFFF,t_70) 



```python
wb = xlwt.Workbook(encording='utf-8',style_compression=0)
sh=wb.add_sheet('moviesTop250',cell_overwrite_ok=True)
col = ('表格的表头名')
for i in range(0,8):
	sh.write(0,i,col[i])
#通过for循环将列表中的数据逐一填写到Excel单元格内
for i in range(0,len(dataList)):
    print('%d'%(i+1))
    data= dataList[i]
    for j in range(0,8):
		sh.write(i+1,j,data[j])
sFile = 'D:/360DownLoad:\doubanmovies.xls'
wb.save(sFile)

```

#### 文件处理：

```python
from openpyxl import Workbook

wb= Workbook()	#创建表
ws= wb.active	#获取当前sheet表
sheet.title = '表名'	#改sheet名

wb2 = load_workbook('文件名字'.xlsx)


#写数据
#method1：直接分配到单元格
sheet["C5"] = 'text'
#method2：附加行
sheet.append([1,2,3])
#method3：自动类型转换
sheet['A3']=''
```



### 正则表达式

#### 常用匹配操作：

1.^符和&符的使用，不要单独使用，结合文本 例如：^He(\d+)Demo$

2.省略的部分书写(非贪婪模式)：(.*?) 

3.遇到不规则的字符：加一个反斜杠  例如：\（百度）www...  避免开始匹配不到

#### 修饰符

```
re.I：使匹配对大小写不敏感
re.L：做本地化识别
re.M：多行匹配，影响^和&
re.S：使 . 符匹配包括换行符在内的所有字符
re.U：根据Unicode字符集解析字符，这个标志影响\w,\W,\b和\B
re.X：给予你更灵活的格式以便于你将正则表达式写得更容易理解
```

#### 模式

```
match():从字符串起始的位置匹配正则表达式，如果匹配就返回，没有匹配就返回None
	用.group(索引)来获取匹配到的内容
search():match方法在开头不匹配可能会终止，search可以避免这个问题，sreach可以搜索整个			字符串
sub():不多说，替换
compile():规定一个同一个的正则表达式，可以重复的使用
```



#### 爬虫中匹配

```
匹配标签中的内容：
	re.compile(<li .*?active.*?singer="(.*?)"> (.*?)</a>,html.re.S)
处理字符串：
	s = re.sub('<a.*?>|</a>','',re.S)  去除标签
	s.strip()
	
```

字符匹配：

1. \d:代表任意数字  \D：代表不是数字的（大写的字母一般是和小写的唱反调）

2. \w：代表字母，数字，下划线，也就是a-z，A-Z，0-9，—。     \W：不是字母数字下划线的

3. \n：代表一个换行

4. \r：代表一个回车      \f：代表一个换页       \t：代表一个tab   

5. \s：代表所有的空白字符（包括换行回车换页tab)          \S：不是空白的字符

6. \A:代表字符串的开始     \Z：代表字符串的结束

7. **^:匹配字符串的开始位置     &：匹配字符串的结束位置**    

8. \b:匹配一个单词的边界          \B：匹配非单词边界

9. |：匹配两边的表达式 +（）                   **要想匹配的不多不少刚刚好，就必须用^和&来规定开始结束**

10. 星号：匹配前面的字符串零次或多次 例如：zo* 能匹配z、zo、zoo

11. +：匹配前面的字符串一次或多次      例如：zo+能匹配zo、zoo但不能匹配z

12. ？：匹配前面的字符串零次或一次    例如：do（es）？可以匹配do或者does

13. [n]：n是一个非负整数，匹配确定的n次   例如o[2]必须匹配到2个o，bob就不能匹配到

14. [n，]：就是至少匹配n次，可以多不能少   [n,m]：最少匹配n次，最多匹配m次

15.    .   :  匹配 \r  \n 之外的任何的单个字符

16. [...]：表示一个范围内的字符 例如[a-z]就是a-z间任意一个字符    **[^]唱反调**

17. {n}：匹配在{n}前面的东西 n次

18. ()括号可以作为一个分组 ，括号括起来的内容可以作为中间变量记录下来，要想记录下来并使用把后面的那一部分也括起来然后写  （\1）数字就是第几个括号

    括号一多容易混，所以用信息加以定义 (？p=< 定义名字>)   例子：(？p=< key1>)

    **首先导入模块re**

    校验数字：

    ![1611386525311](C:\Users\银晗\AppData\Roaming\Typora\typora-user-images\1611386525311.png)

    校验字符的表达式：

    ![1611386578655](C:\Users\银晗\AppData\Roaming\Typora\typora-user-images\1611386578655.png)

    

    ![1611386615056](C:\Users\银晗\AppData\Roaming\Typora\typora-user-images\1611386615056.png)

    re模块的方法：

    1. 循环所有匹配：re.findall()

       re.findall(pattern, string , flags=0)  

       pattern=正则表达式    string=匹配的字符串

       res=re.findall(r‘ r[ua]n’, ‘run ran ren’ )   print(res) 输出run 和ran 

       注意如果要找单独的单词，请在要匹配的单词首尾各空一个空格

    2. re.match(正则表达式，要匹配的字符串)   result=re.match()   从左到右匹配字符串，只返回匹配到的

       如果match匹配到数据用result.group() 返回数据

    3. 替换匹配内容：re.sub()

       re.sub( 待替换的字符串,要替换上的字符串，原来整个字符串)

       过滤掉网页中那些不需要的符号：re.sub( r‘<.+?>’,  ,s)或者(r’</? \w+ >‘ )

       实例操作：re.sub(r “ http://.+?/)”,lambda x: x.group(1),s)  用匿名函数输出来替换

    4. re.search(r’‘  搜索内容’, ‘匹配内容 ’)  :与match不同终端无法继续

    5. 分裂内容：re.split()

       res=re.split(r ‘ ,;\.\ \（分裂标志）’  ,  ‘a,b;c.d\e’)   就是把，；. \之间的值全部分裂开输出a,b,c,d,e

       返回的是列表 

    6. 包装正则表达式：re.compile()   

       compile_re=re.complie(r’ r[ua]n ’)   就相当于等效了一下

       res=compile_re.findall(‘ run ran ren’)

    7. 贪婪模式：只有不设置限制，系统默认一直往后找 

       用括号括起要关闭贪婪模式的表达式 加上一个？

### JSon

json模块是Python自带的模块，用于json与python数据之间的相互转换

 json.load()方法：

1. 把json字符串，转换为python数据

   json_str=‘’‘ ...  str  ...’‘’

   rs = json.loads(json_str)

2. 读取文件：

   with open(‘data.json’)	as fp:

   python_list = json.load(fp)

   

**python 数据转换为json数据**：

json.dumps(obj)  转换为json字符串

json_str = json.dumps(rs, ensure_ascii=Flase)    		#有中文就要指定ensure_ascii=Flase

**python 数据转换为json数据写入文件**：

```python
with open('data.json', 'w') as fp:
    json.dump(rs, fp, ensure_ascii=Flase)
    
```

## 文件存储：

1.json文件存储

```python
with open(file = 'result.txt',mode='a',encording='utf-8') as f:
	f.write(json.dumps(content,ensure_ascii=False)+'\n',indent=2)
    #indent表示缩进2个空格
```

2.csv文件存储

```
import csv
with open('data.csv','w') as csvfile:
	writer.writerow(['','',''])  //写入每行的数据
	
```

**不过我还是建议用pandas中转保存！**

3.连接MySql数据库：

```python
import pymysql
db = pymqsql.connect(host='localhost',user='root',password='20020520zyh,port=3306')
cursor = db.cursor() //获得mysql的操作游标
// 用execute执行sql语句
cursor.execute('SELECT VERSION()')//获取版本
data = cursor.fetchone()
print('DATABASE version:',data)
cursor.execute("Create database spiders default character set utf-8")
db.close()

```



## 高性能异步爬虫：

目的：在爬虫中使用异步实现高性能的数据的爬取

```python
#同步爬虫，单线程，get()很慢，是一个阻塞的方法
response=requests.get(url=url,headers=headers)
if response.status_code == 200:				#判断是否有响应
	return response.content
	
```

异步爬虫方式：

1. 多线程，多进程：可以为相关阻塞的操作单独开启线程或者进程，不会等。但是不能无限制的开启多线程

2. 线程池、进程池：可以降低系统对进程或者线程创建和销毁的一个频率，提升效率。但是池中线程数量有上限

   ```python
   from multiprocessing.dummy import Pool
   
   name_list=['aa','bb','cc','dd']
   def get_page(str):
   {
       print(str)
   }
   #实例化线程对象
   pool=Pool(4)	#四个对象
   pool.map(get_page,name_list)	#map(阻塞的方法，对象列表)
   ```

   

3. 单线程+异步协程：

   1. event_loop:事件循环，相当于无限循环，把函数注册到事件循环上，当满足某些条件时，函数就会被循环执行
   2. coroutine:协程对象，把协程的对象注册到事件循环中，它会被事件循环调用。我们可以使用async关键字来定义一个方法，这个方法在调用时不会立即执行，而是返回一个协程对象。
   3. task:任务，它是对协程对象的封装，包含了任务的各个状态。
   4. future:代表将来执行或者还没执行的任务，和task没有本质区别
   5. async:定义一个协程
   6. await:用来挂起阻塞的方法

```python
import asyncio


async def request(url):
    # 用async修饰的函数，调用后返回是一个协程对象
    print('请求的对象是', url)


c = request('www.baidu.com')

# #创建事件循环对象
# loop = asyncio.get_event_loop()
# #将协程对象注册到loop中，然后启动loop
# loop.run_until_complete(c)

# task的使用
loop = asyncio.get_event_loop()
# 基于loop创建一个task对象
task= loop.create_task(c)
print(task)
loop.run_until_complete(task)
print(task)
#future的使用
loop = asyncio.get_event_loop()
task= asyncio.ensure_future(c)
loop.run_until_complete(task)

# 多任务异步协程实现
urls = {
    'www.baidu.com',
    'www.sougou.com',
    'www.goubanjia.com',
}
stasks = []  # 存放任务列表
for url in urls:
    c = request(url)
    task = asyncio.ensure_future(c)
    stasks.append(task)
    # 需要将任务列表封装到wait中
    loop.run_until_complete(asyncio.wait(stasks))
    # 但是在异步协程中出现同步模块的代码，那么无法实现异步

async def requests(url):
    print('正在下载',url)
    # 改一下time.sleep(2)
    await asyncio.sleep(2)
```



## 数据分析：

### jupyter notebook：

基本操作快捷键：

1. 添加cell：a或者b
2. 删除cell：x
3. 修改成markdown模式：m
4. 修改成code模式：y
5. 执行cell：shift+enter
6. 自动补全：tab
7. 打开帮助文档：shift+tab



## Selenium

之前动态加载数据，需要用抓包工具，抓XHR中的参数，用param参数列表进行爬取，selenium更方便

**selenium是基于自动化的一个模块**

基本使用：

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_condition as EC
from selenium.webdriver.support.wait import WebDriverWait

browser = webdriver.Chrome()
try:
    browser.get('https://www.baidu.com') #访问页面
    input = broswer.find_element_by_id('kw') #找到输入框
    input.send_keys('Python') #输入
    input.send_keys(Keys.ENTER) #按回车
    wait = WebdriverWait(broswer,10)  #等待10s
    wait.until(EC.presence_of_element_located((By.ID),'content_left'))
    
    print(broswer.current_url,
         broswer.get_cookies(),
         broswe.page_source())
expect Expection as e:
    print(e)
Fianlly:
    broswer.close()
```

自动化代码：

1. 发起请求：get（）

2. broswer=  webdriver.Edge()

3. 标签定位：find_element_by_选择器(）

4. 标签交互：send_keys()

5. 执行js程序：excute_script（‘jsCode’）

6. 前进、后退：back()、forward（）

7. 关闭：quit()

8. 点击：click()

9. 清空：clear()

10. 获取cookies:   

     

11. ```python
    #选项卡管理
    bro.get(...)
    bro.execute_script('window.open()')
    bro.switch_to_window(bro.window_handles[1])
    bro.get(...)
    bro.switch_to_window(bro.window_handles[0])
    
    window.open()是JS语句，开启一个选项卡
    bro.window_handles :获取当前开启的所有选项卡
    switch_to_window(选项卡代号)：切换选项卡
    ```

12. 异常处理

    ![1632557905053](C:%5CUsers%5C%E9%93%B6%E6%99%97%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1632557905053.png)

    

#### 标签选择器：

> 单个标签：find_element_by_选择器（）

选择器：id	name	xpath	link_text	tag_name	class_name	css_selector

> 多个标签：find_elements_by_选择器()

#### 结点交互：

输入：send_keys(‘内容’)

前进、后退：back()、forward（）

关闭：quit()

点击：click()

清空：clear()

#### 获取结点信息：

通过page_source可以获取网页的源代码，接着可以用解析库解析了，**但是selenium提供了选择节点的方法**

例：

```python
bro = webdriver.Chrome()
url = 'https://zhihu.com'
bro.get(url)
input = bro.find_element_by_class_name('zu-top-add-question')
print(input.text)
```

获取结点的信息主要通过属性：

```
input.id
input.location
input.tag_name
input.size
```

#### 切换frame：

网页中有一种节点叫做iframe , 也就是子Frame,相当于页面的子页面，它的结构和外部页面一致，但是selenium打开页面之后默认在父级Frame里面操作，不能获取子Frame里面的节点，这时候需要swith_to.frame()来切换Frame

```
browser.get(url) 
browser.switch_to.frame('iframresult ’) 
browser.switch_to.parent_frame()
```

#### 等待：

隐式等待：Selenium没有找到节点，就等着直到超出设定时间，则抛出异常

```
browser=webdriver.Edge()
browser=implicitly_wait(10) 等待10s
```

显式等待：指定最长等待时间，如果在规定时间内加载出来就返回，超时则抛出异常

```
wait = WebDriverWait(browser,10)
input = wait.until(EC.presence_of_element_located(By.ID,'q'))
```

![1632556633791](C:%5CUsers%5C%E9%93%B6%E6%99%97%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1632556633791.png)

![1632556869868](C:%5CUsers%5C%E9%93%B6%E6%99%97%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1632556869868.png)

#### cookies:

```python
.get_cookies()   

.add_cookies({‘name’:  ,‘domain’:  ,‘values’:  }) 

 delete_all_cookies()
```

#### 选项卡管理：

```python
bro.get(...)
bro.execute_script('window.open()')
bro.switch_to_window(bro.window_handles[1])
bro.get(...)
bro.switch_to_window(bro.window_handles[0])

window.open()是JS语句，开启一个选项卡
bro.window_handles :获取当前开启的所有选项卡（列表）
switch_to_window(选项卡代号)：切换选项卡
```



### 无头浏览器设置：

```python
rom selenium import webdriver
from selenium.webdriver.edge.options import Options
from time import sleep

#创建无头浏览器对象
edge_options = Options()
path = "MicrosoftWebDriver.exe"
EDGE = {
    "browserName": "MicrosoftEdge",
    "version": "",
    "platform": "WINDOWS",
    "ms:edgeOptions": {
        'extensions': [],
        'args': [
            '--headless',
            '--disable-gpu'
        ]}
}
bro = webdriver.Edge(executable_path=path,capabilities=EDGE)

```



#### iframe:

```python
#切换浏览器的标签作用域：
bro.switch_to.frame(‘iframeResult’)//切换到子页面
bro.switch_to.parent_frame()

div=bro.find_element_by_id('id')

#动作链
action= ActionChains(bro)
#点击长按指定标签
action.click_and_hold(div)

for i in range(5):
    action.move_by_offset(17).perform()	#perform执行动作链，移动操纵
    sleep(0.3)
action.release()	#释放动作链
    
   
```



```
隐式等待：
browser=webdriver.Edge()
browser=implicitly_wait(10) 等待10s

显式等待：指定最长等待时间，如果在规定时间内加载出来就返回，超时则抛出异常
wait = WebDriverWait(browser,10)
input = wait.until(EC.presence_of_element_located(By.ID,'q'))

```

![1632556633791](C:%5CUsers%5C%E9%93%B6%E6%99%97%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1632556633791.png)

![1632556869868](C:%5CUsers%5C%E9%93%B6%E6%99%97%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1632556869868.png)





## scrapy：

创建工程：scrapy startproject 文件名

​					cd 文件名

在spiders子目录中创建一个爬虫文件：scrapy genspider first  www.xxx.com

执行工程：scrapy crawl 文件名

```
进入setting：

进行UA伪装，USER_AGENT='...'
把ROBOTSTXT_OBEY = True 改成Flase 
添加LOG_LEVEL='ERROR'

```

```python
import scrapy


class FirstSpider(scrapy.Spider):
    name = 'first'  # 爬虫文件的名称
    # allowed_domains = ['www.xxx.com']   允许的域名，start_urls中哪些url可以进行请求，不过一般不用这种机制
    start_urls = ['https://www.baidu.com/', 'https://www.sogou.com/']  # 起始的url列表,可以有多个url

    # 用于数据解析，response参数表示就是start_urls列表的url请求成功后对应响应的对象
    def parse(self, response):
        pass
```

#### **持久化存储：**

```python
#基于终端指令：只可以将parse方法的返回值存储到本地的文本文件中
scrapy crawl 爬虫名 -o ./文件名.csv
#基于管道：在items.py中添加文本对象
文本对象名= scrapy.Field()
#再在pipelines.py中process_item接收持久化储存对象
class QiubaiproPipeline:
    fp= None
    def open_spider(self,spider):
        self.fp= open('./qiubai.txt','w',encoding='utf-8')
		
    def process_item(self, item, spider):
        author= item['author']				#定义属性
        page_text= item['page_text']		#定义属性	
        self.fp.write(author+':'+page_text+'\n')

        return item	#传递下一个管道类，养成习惯
    
    def close_spider(self,spider):
        self.fp.close()
#一个管道类存储一份数据

        
        
        
        
        
        
#再去setting第65-68行开启管道

#爬虫主程序

from qiubaiPro.items import QiubaiproItem
#把工程中items.py文件中 类名导入
item= QiubaiproItem()			
item['author'] = author			#item回倒主程序
item['page_text'] = page_text	#item回倒主程序
yield item



#直接写到mysql数据库:

#重写一个管道类

import pymysql

class mysqlPileLine(object):
    conn= None
    cursor= None
    def open_spider(self,spider):
        #链接对象
        self.conn= pymysql.Connect(host='127.0.0.1',port=3306,user='root',password='20020520zyh',db=qubai,charset='utf8')

    def process_item(self,item,spider):
        #创建游标对象
        self.cursor = self.conn.cursor()

        try:
            self.cursor.execute('insert into qubai values ("%s","%s")'%(item["author"],item["page_text"]))
            self.conn.commit()  #提交
        except Exception as e:
            print('e')
            self.conn.rollback()
		return item
    def close_spider(self,spider):
        self.cursor.close()
        self.conn.close()

#去setting里添加管道
TEM_PIPELINES = {
   'qiubaiPro.pipelines.QiubaiproPipeline': 300,
   'qiubaiPro.pipelines.mysqlPileLine': 301,
}


```

#### 循环爬取网站多页数据：

```python
class HanSpider(scrapy.Spider):
    name = 'han'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['http://www.xxx.com/']
    # 通用url模板
    url = '...%d...'
    page_num = 2

    def parse(self, response):
        li_list = response.xpath(' ')
        for li in li_list:
            page_text = li.xpath('... | ... ').extract_first()  # 出现数据为空，直接去找空的情况对应的xpath路径，用或连接
            print(page_text)
            if self.page_num <=11:
                new_url = format(self.url % self.page_num)
                self.page_num+=1
                yield scrapy.Request(url=new_url, callback=self.parse())    #手动发送请求，callback回调函数专门用于数据解析
```



#### 深度爬取：

```python
#第二层网页的爬取
    def parse_detail(self, response):
        # 接收参数
        item = response.meta['item']

        job_detail = response.xpath('//*[@id="main"]/div[3]/div/div[2]/div[2]/div[1]/div//text()').extract_first()
        job_detail = ''.join(job_detail)
        print(job_detail)
        item['job_detail'] = job_detail
        yield item  # 提交管道

def parse(self, response):
        li_list = response.xpath(' ')
        for li in li_list:
            page_text = li.xpath('... | ... ').extract_first() 
            # 出现数据为空，直接去找空的情况对应的xpath路径，用或连接
            item['job'] = job
            item['job_area'] = job_area
            
            # 请求传参
            yield scrapy.Request(detail_url, callback=self.parse_detail, meta={'item': item}, dont_filter=True)
```

#### 图片爬取：

```python
字符串数据：只需要基于xpath进行解析，且提交管道进行持久化存储
图片：xpath解析出图片的src的属性值，单独的对图片地址发起请求获取图片二进制类型的数据

ImagesPipeline:只需要将img的src的属性值解析进行解析，提交到管道，管道就会对src进行请求发送获取图片的二进制数据
    
    
1.xpath解析图片地址
2.将存储图片地址的item存储到制定管道类
 src = li.xpath('./div/a/img/@src2').extract_first()
            src = 'https:'+src
            # print(src)
            item = ImgsproItem()
            item['src'] = src
            yield item
3.重写管道类

from scrapy.pipelines.images import ImagesPipeline
import scrapy
class imagesPileLine(ImagesPipeline):

    #对图片进行请求操作
    def get_media_requests(self, item, info):


        yield scrapy.Request(item['src'])

    #指定图片的存储路径
    def file_path(self, request, response=None, info=None):
        imgName = request.url.split('/')[-1]
        return imgName

    def item_completed(self, results, item, info):
        return item  #返回给下一个即将被执行的管道类，养成习惯
    
4.进入setting更改配置：

开启管道类，并修改类名，改为自定义的管道类名

末尾增加一行存储路径
IMAGES_STORE = './imgs_han'
```



#### 中间件：

```python
middlewares.py文件中
# import random

	网上找一些ip复制进来
    # user_agent_list=[
    #     '...'
    # ]
    # PROXY_http = [
    #     '...'
    # ]
    #
    # PROXY_https = [
    #     '...'
    # ]
    def process_request(self, request, spider):
        # request.headers['User-Agent'] = random.choice(self.user_agent_list)
        return None

    def process_response(self, request, response, spider):

        return response

    def process_exception(self, request, exception, spider):

        # if request.url.split(':')[0] == 'http':
        #     request.meta['proxy'] = 'http://'+random.choice(self.PROXY_http)
        #
        # else:
        # #代理
        # request.meta['proxy'] = 'https://'+random.choice(self.PROXY_http)
        #
        # return request
       

    
    setting文件中：第55行开启中间件
#DOWNLOADER_MIDDLEWARES = {
#    'imgsPro.middlewares.ImgsproDownloaderMiddleware': 543,
#}
```



#### CrawlSpider：

全站数据爬取的方式：基于spider：手动请求；基于CrawlSpider

使用：创建爬虫文件，scrapy genspider -t  crawl 文件名 www.xxx.com

```python
 #链接提取器
    link = LinkExtractor(allow=r'type=4&page=\d+')#匹配到了当前页面的链接
#去起始url中，根据指定规则，提取链接  allow="正则表达式"
    rules = (
        #规则解析器对象
        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
    )
    
```

全站数据爬取过程：

1. 可以使用链接提取器提取所有的页码链接
2. 让链接提取器提取所有的新闻详情页链接



c