{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手把手打一场NLP赛事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然语言处理（Natural Language Processing，NLP）是指计算机处理和理解人类语言的技术。NLP涵盖了从文本的基本语法和词汇处理到更高级的任务，如机器翻译、情感分析、问答系统等。NLP利用计算机算法和模型，对文本进行分词、词性标注、句法分析等处理，以便将人类语言转化为计算机可以理解和处理的形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 赛题信息\n",
    "医学领域的文献库中蕴含了丰富的疾病诊断和治疗信息，如何高效地从海量文献中提取关键信息，进行疾病诊断和治疗推荐，对于临床医生和研究人员具有重要意义。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于论文摘要的文本分类与关键词抽取挑战赛  \n",
    "https://challenge.xfyun.cn/topic/info?type=abstract-of-the-paper&ch=ymfk4uU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/bc8c545638eb4200a68836ed741b6fe7d75108e9009d443b8de5b33fb8e0fa55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.准备步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 赛事报名\n",
    "赛事地址：https://challenge.xfyun.cn/topic/info?type=abstract-of-the-paper&ch=ZuoaKcY\n",
    "1. 点击报名参赛，登录讯飞开放平台。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 数据下载\n",
    "数据已提前下载在数据集目录下，您可以自行查看其中的train与test文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 环境配置参考资料\n",
    "python环境的搭建请参考：\n",
    "- [Mac设备：Mac上安装Anaconda最全教程](https://zhuanlan.zhihu.com/p/350828057)\n",
    "- [Windows设备：Anaconda超详细安装教程(Windows环境下)_菜鸟1号!!的博客-CSDN博客_windows安装anaconda](https://blog.csdn.net/fan18317517352/article/details/123035625)\n",
    "Jupyter 编辑器的使用请参考：\n",
    "- [Jupyter Notebook最全使用教程，看这篇就够了！](https://www.jianshu.com/p/6cc047bc94e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 赛题解析\n",
    "实践任务\n",
    "本任务分为两个子任务：\n",
    "1. 从论文标题、摘要作者等信息，判断该论文是否属于医学领域的文献。\n",
    "2. 从论文标题、摘要作者等信息，提取出该论文关键词。\n",
    "\n",
    "第一个任务看作是一个文本二分类任务。机器需要根据对论文摘要等信息的理解，将论文划分为医学领域的文献和非医学领域的文献两个类别之一。第二个任务看作是一个文本关键词识别任务。机器需要从给定的论文中识别和提取出与论文内容相关的关键词。  \n",
    "\n",
    "**本次学习中我们仅学习第一个任务**  \n",
    "\n",
    "数据集解析\n",
    "训练集与测试集数据为CSV格式文件，各字段分别是标题、作者和摘要。Keywords为任务2的标签，label为任务1的标签。训练集和测试集都可以通过pandas读取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/8c88537bce9d46049151389396c4c5b828556ba332d34ed3a24555e4e28e7191)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.实践思路&baseline\n",
    "### 实践思路\n",
    "本赛题任务主要如下：\n",
    "1. 从论文标题、摘要作者等信息，判断该论文是否属于医学领域的文献。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务一：文本二分类\n",
    "第一个任务看作是一个文本二分类任务。机器需要根据对论文摘要等信息的理解，将论文划分为医学领域的文献和非医学领域的文献两个类别之一。\n",
    "\n",
    "针对文本分类任务，可以提供两种实践思路，一种是使用传统的特征提取方法（如TF-IDF/BOW）结合机器学习模型，另一种是使用预训练的BERT模型进行建模。使用特征提取 + 机器学习的思路步骤如下：\n",
    "1. 数据预处理：首先，对文本数据进行预处理，包括文本清洗（如去除特殊字符、标点符号）、分词等操作。可以使用常见的NLP工具包（如NLTK或spaCy）来辅助进行预处理。\n",
    "2. 特征提取：使用TF-IDF（词频-逆文档频率）或BOW（词袋模型）方法将文本转换为向量表示。TF-IDF可以计算文本中词语的重要性，而BOW则简单地统计每个词语在文本中的出现次数。可以使用scikit-learn库的TfidfVectorizer或CountVectorizer来实现特征提取。\n",
    "3. 构建训练集和测试集：将预处理后的文本数据分割为训练集和测试集，确保数据集的样本分布均匀。\n",
    "4. 选择机器学习模型：根据实际情况选择适合的机器学习模型，如朴素贝叶斯、支持向量机（SVM）、随机森林等。这些模型在文本分类任务中表现良好。可以使用scikit-learn库中相应的分类器进行模型训练和评估。\n",
    "5. 模型训练和评估：使用训练集对选定的机器学习模型进行训练，然后使用测试集进行评估。评估指标可以选择准确率、精确率、召回率、F1值等。\n",
    "6. 调参优化：如果模型效果不理想，可以尝试调整特征提取的参数（如词频阈值、词袋大小等）或机器学习模型的参数，以获得更好的性能。  \n",
    "\n",
    "\n",
    "Baseline中我们选择使用BOW将文本转换为向量表示，选择逻辑回归模型来完成训练和评估  \n",
    "代码演示如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The repository located at mirrors.aliyun.com is not a trusted or secure host and is being ignored. If this repository is available via HTTPS we recommend you use HTTPS instead, otherwise you may silence this warning and allow it anyway with '--trusted-host mirrors.aliyun.com'.\n",
      "ERROR: Could not find a version that satisfies the requirement pandas (from versions: none)\n",
      "ERROR: No matching distribution found for pandas\n",
      "WARNING: The repository located at mirrors.aliyun.com is not a trusted or secure host and is being ignored. If this repository is available via HTTPS we recommend you use HTTPS instead, otherwise you may silence this warning and allow it anyway with '--trusted-host mirrors.aliyun.com'.\n"
     ]
    }
   ],
   "source": [
    "# 获取前置依赖\n",
    "#!pip install nltk\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入pandas用于读取表格数据\n",
    "import pandas as pd\n",
    "\n",
    "# 导入BOW（词袋模型），可以选择将CountVectorizer替换为TfidfVectorizer（TF-IDF（词频-逆文档频率）），注意上下文要同时修改，亲测后者效果更佳\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 导入LogisticRegression回归模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 过滤警告消息\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# 读取数据集\n",
    "train = pd.read_csv('./data/train.csv')  # 读取训练集数据\n",
    "train['title'] = train['title'].fillna('')  # 填充训练集中的'title'列的缺失值为空字符串\n",
    "train['abstract'] = train['abstract'].fillna('')  # 填充训练集中的'abstract'列的缺失值为空字符串\n",
    "\n",
    "test = pd.read_csv('./data/testB.csv')  # 读取测试集数据\n",
    "test['title'] = test['title'].fillna('')  # 填充测试集中的'title'列的缺失值为空字符串\n",
    "test['abstract'] = test['abstract'].fillna('')  # 填充测试集中的'abstract'列的缺失值为空字符串\n",
    "\n",
    "# 提取文本特征，生成训练集与测试集\n",
    "train['text'] = train['title'].fillna('') + ' ' +  train['author'].fillna('') + ' ' + train['abstract'].fillna('')+ ' ' + train['Keywords'].fillna('')  # 将训练集的'title'、'author'、'abstract'和'Keywords'列的文本内容合并成一列，存储在'text'列中\n",
    "test['text'] = test['title'].fillna('') + ' ' +  test['author'].fillna('') + ' ' + test['abstract'].fillna('')  # 将测试集的'title'、'author'和'abstract'列的文本内容合并成一列，存储在'text'列中\n",
    "\n",
    "vector = CountVectorizer().fit(train['text'])  # 使用CountVectorizer提取文本特征，拟合训练集中的文本数据\n",
    "train_vector = vector.transform(train['text'])  # 将训练集的文本数据转换为特征矩阵表示\n",
    "test_vector = vector.transform(test['text'])  # 将测试集的文本数据转换为特征矩阵表示\n",
    "\n",
    "# 引入模型\n",
    "model = LogisticRegression()  # 初始化一个LogisticRegression模型\n",
    "\n",
    "# 开始训练，这里可以考虑修改默认的batch_size与epoch来取得更好的效果\n",
    "model.fit(train_vector, train['label'])  # 使用训练集的特征矩阵和标签训练模型\n",
    "\n",
    "# 利用模型对测试集label标签进行预测\n",
    "test['label'] = model.predict(test_vector)  # 使用训练好的模型对测试集的特征矩阵进行预测，并将预测结果存储在测试集的'label'列\n",
    "test['Keywords'] = test['title'].fillna('')  # 填充测试集中的'title'列的缺失值为空字符串\n",
    "test[['uuid', 'Keywords', 'label']].to_csv('submit_task1.csv', index=None)  # 将测试集的'uuid'、'Keywords'和'label'列保存到一个CSV文件中，不包含行索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MiniConda\\envs\\pytorch39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "inputs: {'input_ids': tensor([[  101,  1037,  3016,  ...,  3259,  4895,   102],\n",
      "        [  101, 13075,  3593,  ...,  4316,  1010,   102],\n",
      "        [  101,  2522, 24879,  ..., 23320, 10631,   102],\n",
      "        ...,\n",
      "        [  101,  3435,  3565,  ...,  1012,  2023,   102],\n",
      "        [  101,  1043,  9863,  ...,  6534,  2075,   102],\n",
      "        [  101,  5192,  1011,  ...,  5192, 15476,   102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "targets: tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1])\n",
      "Epoch 1/50, Step: 49/338, total loss:14.6007\n",
      "Epoch 1/50, Step: 99/338, total loss:7.1087\n",
      "Epoch 1/50, Step: 149/338, total loss:5.9167\n",
      "Epoch 1/50, Step: 199/338, total loss:5.4029\n",
      "Epoch 1/50, Step: 249/338, total loss:5.1587\n",
      "Epoch 1/50, Step: 299/338, total loss:5.6989\n",
      "Epoch 1, accuracy: 0.9333, validation loss: 0.0098\n",
      "Epoch 2/50, Step: 11/338, total loss:4.7825\n",
      "Epoch 2/50, Step: 61/338, total loss:4.8857\n",
      "Epoch 2/50, Step: 111/338, total loss:3.6039\n",
      "Epoch 2/50, Step: 161/338, total loss:4.3129\n",
      "Epoch 2/50, Step: 211/338, total loss:2.8787\n",
      "Epoch 2/50, Step: 261/338, total loss:3.7318\n",
      "Epoch 2/50, Step: 311/338, total loss:3.3252\n",
      "Epoch 2, accuracy: 0.9500, validation loss: 0.0079\n",
      "Epoch 3/50, Step: 23/338, total loss:2.2604\n",
      "Epoch 3/50, Step: 73/338, total loss:3.1536\n",
      "Epoch 3/50, Step: 123/338, total loss:2.9016\n",
      "Epoch 3/50, Step: 173/338, total loss:2.3436\n",
      "Epoch 3/50, Step: 223/338, total loss:1.6852\n",
      "Epoch 3/50, Step: 273/338, total loss:2.4113\n",
      "Epoch 3/50, Step: 323/338, total loss:2.7417\n",
      "Epoch 3, accuracy: 0.9667, validation loss: 0.0055\n",
      "Epoch 4/50, Step: 35/338, total loss:2.6506\n",
      "Epoch 4/50, Step: 85/338, total loss:2.2499\n",
      "Epoch 4/50, Step: 135/338, total loss:1.5455\n",
      "Epoch 4/50, Step: 185/338, total loss:2.3860\n",
      "Epoch 4/50, Step: 235/338, total loss:1.7444\n",
      "Epoch 4/50, Step: 285/338, total loss:0.7889\n",
      "Epoch 4/50, Step: 335/338, total loss:1.5745\n",
      "Epoch 4, accuracy: 0.9550, validation loss: 0.0095\n",
      "Epoch 5/50, Step: 47/338, total loss:0.4079\n",
      "Epoch 5/50, Step: 97/338, total loss:0.4886\n",
      "Epoch 5/50, Step: 147/338, total loss:1.6564\n",
      "Epoch 5/50, Step: 197/338, total loss:1.3555\n",
      "Epoch 5/50, Step: 247/338, total loss:1.5619\n",
      "Epoch 5/50, Step: 297/338, total loss:2.6081\n",
      "Epoch 5, accuracy: 0.9617, validation loss: 0.0060\n",
      "Epoch 6/50, Step: 9/338, total loss:1.7301\n",
      "Epoch 6/50, Step: 59/338, total loss:0.4772\n",
      "Epoch 6/50, Step: 109/338, total loss:0.3722\n",
      "Epoch 6/50, Step: 159/338, total loss:0.6196\n",
      "Epoch 6/50, Step: 209/338, total loss:0.4532\n",
      "Epoch 6/50, Step: 259/338, total loss:1.3132\n",
      "Epoch 6/50, Step: 309/338, total loss:1.4119\n",
      "Epoch 6, accuracy: 0.9633, validation loss: 0.0100\n",
      "Epoch 7/50, Step: 21/338, total loss:1.8607\n",
      "Epoch 7/50, Step: 71/338, total loss:0.8083\n",
      "Epoch 7/50, Step: 121/338, total loss:1.4483\n",
      "Epoch 7/50, Step: 171/338, total loss:0.6510\n",
      "Epoch 7/50, Step: 221/338, total loss:1.0000\n",
      "Epoch 7/50, Step: 271/338, total loss:0.5185\n",
      "Epoch 7/50, Step: 321/338, total loss:0.8063\n",
      "Epoch 7, accuracy: 0.9600, validation loss: 0.0119\n",
      "Epoch 8/50, Step: 33/338, total loss:0.3686\n",
      "Epoch 8/50, Step: 83/338, total loss:0.3438\n",
      "Epoch 8/50, Step: 133/338, total loss:0.5230\n",
      "Epoch 8/50, Step: 183/338, total loss:0.5194\n",
      "Epoch 8/50, Step: 233/338, total loss:0.2180\n",
      "Epoch 8/50, Step: 283/338, total loss:0.1787\n",
      "Epoch 8/50, Step: 333/338, total loss:1.4835\n",
      "Epoch 8, accuracy: 0.9467, validation loss: 0.0130\n",
      "Epoch 9/50, Step: 45/338, total loss:0.3650\n",
      "Epoch 9/50, Step: 95/338, total loss:0.6103\n",
      "Epoch 9/50, Step: 145/338, total loss:0.0561\n",
      "Epoch 9/50, Step: 195/338, total loss:0.0641\n",
      "Epoch 9/50, Step: 245/338, total loss:0.0665\n",
      "Epoch 9/50, Step: 295/338, total loss:0.0888\n",
      "Epoch 9, accuracy: 0.9517, validation loss: 0.0145\n",
      "Epoch 10/50, Step: 7/338, total loss:1.4659\n",
      "Epoch 10/50, Step: 57/338, total loss:0.8825\n",
      "Epoch 10/50, Step: 107/338, total loss:0.0843\n",
      "Epoch 10/50, Step: 157/338, total loss:1.7121\n",
      "Epoch 10/50, Step: 207/338, total loss:0.8447\n",
      "Epoch 10/50, Step: 257/338, total loss:0.2844\n",
      "Epoch 10/50, Step: 307/338, total loss:0.1878\n",
      "Epoch 10, accuracy: 0.9633, validation loss: 0.0151\n",
      "Epoch 11/50, Step: 19/338, total loss:0.0145\n",
      "Epoch 11/50, Step: 69/338, total loss:0.0122\n",
      "Epoch 11/50, Step: 119/338, total loss:0.0416\n",
      "Epoch 11/50, Step: 169/338, total loss:0.0089\n",
      "Epoch 11/50, Step: 219/338, total loss:0.0232\n",
      "Epoch 11/50, Step: 269/338, total loss:0.0210\n",
      "Epoch 11/50, Step: 319/338, total loss:0.0490\n",
      "Epoch 11, accuracy: 0.9650, validation loss: 0.0126\n",
      "Epoch 12/50, Step: 31/338, total loss:1.0239\n",
      "Epoch 12/50, Step: 81/338, total loss:0.6285\n",
      "Epoch 12/50, Step: 131/338, total loss:0.0563\n",
      "Epoch 12/50, Step: 181/338, total loss:0.0322\n",
      "Epoch 12/50, Step: 231/338, total loss:0.0081\n",
      "Epoch 12/50, Step: 281/338, total loss:0.2500\n",
      "Epoch 12/50, Step: 331/338, total loss:0.6632\n",
      "Epoch 12, accuracy: 0.9600, validation loss: 0.0150\n",
      "Epoch 13/50, Step: 43/338, total loss:0.0459\n",
      "Epoch 13/50, Step: 93/338, total loss:0.6751\n",
      "Epoch 13/50, Step: 143/338, total loss:2.5562\n",
      "Epoch 13/50, Step: 193/338, total loss:0.2020\n",
      "Epoch 13/50, Step: 243/338, total loss:0.3448\n",
      "Epoch 13/50, Step: 293/338, total loss:1.8627\n",
      "Epoch 13, accuracy: 0.9683, validation loss: 0.0104\n",
      "Epoch 14/50, Step: 5/338, total loss:0.3127\n",
      "Epoch 14/50, Step: 55/338, total loss:0.0740\n",
      "Epoch 14/50, Step: 105/338, total loss:0.0267\n",
      "Epoch 14/50, Step: 155/338, total loss:0.0190\n",
      "Epoch 14/50, Step: 205/338, total loss:0.0498\n",
      "Epoch 14/50, Step: 255/338, total loss:0.0426\n",
      "Epoch 14/50, Step: 305/338, total loss:0.0108\n",
      "Epoch 14, accuracy: 0.9700, validation loss: 0.0129\n",
      "Epoch 15/50, Step: 17/338, total loss:0.0104\n",
      "Epoch 15/50, Step: 67/338, total loss:0.0083\n",
      "Epoch 15/50, Step: 117/338, total loss:0.0071\n",
      "Epoch 15/50, Step: 167/338, total loss:0.0073\n",
      "Epoch 15/50, Step: 217/338, total loss:0.0069\n",
      "Epoch 15/50, Step: 267/338, total loss:0.0066\n",
      "Epoch 15/50, Step: 317/338, total loss:0.0066\n",
      "Epoch 15, accuracy: 0.9700, validation loss: 0.0139\n",
      "Epoch 16/50, Step: 29/338, total loss:0.0053\n",
      "Epoch 16/50, Step: 79/338, total loss:0.0052\n",
      "Epoch 16/50, Step: 129/338, total loss:0.0047\n",
      "Epoch 16/50, Step: 179/338, total loss:0.0060\n",
      "Epoch 16/50, Step: 229/338, total loss:0.0042\n",
      "Epoch 16/50, Step: 279/338, total loss:0.0044\n",
      "Epoch 16/50, Step: 329/338, total loss:0.0043\n",
      "Epoch 16, accuracy: 0.9700, validation loss: 0.0146\n",
      "Epoch 17/50, Step: 41/338, total loss:0.0037\n",
      "Epoch 17/50, Step: 91/338, total loss:0.0036\n",
      "Epoch 17/50, Step: 141/338, total loss:0.0035\n",
      "Epoch 17/50, Step: 191/338, total loss:0.0033\n",
      "Epoch 17/50, Step: 241/338, total loss:0.0033\n",
      "Epoch 17/50, Step: 291/338, total loss:0.0031\n",
      "Epoch 17, accuracy: 0.9700, validation loss: 0.0151\n",
      "Epoch 18/50, Step: 3/338, total loss:0.0028\n",
      "Epoch 18/50, Step: 53/338, total loss:0.0027\n",
      "Epoch 18/50, Step: 103/338, total loss:0.0029\n",
      "Epoch 18/50, Step: 153/338, total loss:0.0025\n",
      "Epoch 18/50, Step: 203/338, total loss:0.0024\n",
      "Epoch 18/50, Step: 253/338, total loss:0.0026\n",
      "Epoch 18/50, Step: 303/338, total loss:0.0024\n",
      "Epoch 18, accuracy: 0.9700, validation loss: 0.0156\n",
      "Epoch 19/50, Step: 15/338, total loss:0.0022\n",
      "Epoch 19/50, Step: 65/338, total loss:0.0022\n",
      "Epoch 19/50, Step: 115/338, total loss:0.0022\n",
      "Epoch 19/50, Step: 165/338, total loss:0.0020\n",
      "Epoch 19/50, Step: 215/338, total loss:0.0020\n",
      "Epoch 19/50, Step: 265/338, total loss:0.0020\n",
      "Epoch 19/50, Step: 315/338, total loss:0.0019\n",
      "Epoch 19, accuracy: 0.9700, validation loss: 0.0160\n",
      "Epoch 20/50, Step: 27/338, total loss:0.0018\n",
      "Epoch 20/50, Step: 77/338, total loss:0.0017\n",
      "Epoch 20/50, Step: 127/338, total loss:0.0017\n",
      "Epoch 20/50, Step: 177/338, total loss:0.0017\n",
      "Epoch 20/50, Step: 227/338, total loss:0.0015\n",
      "Epoch 20/50, Step: 277/338, total loss:0.0016\n",
      "Epoch 20/50, Step: 327/338, total loss:0.0015\n",
      "Epoch 20, accuracy: 0.9700, validation loss: 0.0165\n",
      "Epoch 21/50, Step: 39/338, total loss:0.0014\n",
      "Epoch 21/50, Step: 89/338, total loss:0.0014\n",
      "Epoch 21/50, Step: 139/338, total loss:0.0013\n",
      "Epoch 21/50, Step: 189/338, total loss:0.0014\n",
      "Epoch 21/50, Step: 239/338, total loss:0.0012\n",
      "Epoch 21/50, Step: 289/338, total loss:0.0013\n",
      "Epoch 21, accuracy: 0.9700, validation loss: 0.0169\n",
      "Epoch 22/50, Step: 1/338, total loss:0.0012\n",
      "Epoch 22/50, Step: 51/338, total loss:0.0011\n",
      "Epoch 22/50, Step: 101/338, total loss:0.0011\n",
      "Epoch 22/50, Step: 151/338, total loss:0.0010\n",
      "Epoch 22/50, Step: 201/338, total loss:0.0010\n",
      "Epoch 22/50, Step: 251/338, total loss:0.0011\n",
      "Epoch 22/50, Step: 301/338, total loss:0.0010\n",
      "Epoch 22, accuracy: 0.9700, validation loss: 0.0173\n",
      "Epoch 23/50, Step: 13/338, total loss:0.0010\n",
      "Epoch 23/50, Step: 63/338, total loss:0.0009\n",
      "Epoch 23/50, Step: 113/338, total loss:0.0009\n",
      "Epoch 23/50, Step: 163/338, total loss:0.0008\n",
      "Epoch 23/50, Step: 213/338, total loss:0.0008\n",
      "Epoch 23/50, Step: 263/338, total loss:0.0008\n",
      "Epoch 23/50, Step: 313/338, total loss:0.0008\n",
      "Epoch 23, accuracy: 0.9700, validation loss: 0.0177\n",
      "Epoch 24/50, Step: 25/338, total loss:0.0008\n",
      "Epoch 24/50, Step: 75/338, total loss:0.0007\n",
      "Epoch 24/50, Step: 125/338, total loss:0.0007\n",
      "Epoch 24/50, Step: 175/338, total loss:0.0007\n",
      "Epoch 24/50, Step: 225/338, total loss:0.0007\n",
      "Epoch 24/50, Step: 275/338, total loss:0.0006\n",
      "Epoch 24/50, Step: 325/338, total loss:0.0007\n",
      "Epoch 24, accuracy: 0.9700, validation loss: 0.0180\n",
      "Epoch 25/50, Step: 37/338, total loss:0.0006\n",
      "Epoch 25/50, Step: 87/338, total loss:0.0006\n",
      "Epoch 25/50, Step: 137/338, total loss:0.0006\n",
      "Epoch 25/50, Step: 187/338, total loss:0.0006\n",
      "Epoch 25/50, Step: 237/338, total loss:0.0005\n",
      "Epoch 25/50, Step: 287/338, total loss:0.0005\n",
      "Epoch 25/50, Step: 337/338, total loss:0.0005\n",
      "Epoch 25, accuracy: 0.9700, validation loss: 0.0184\n",
      "Epoch 26/50, Step: 49/338, total loss:0.0005\n",
      "Epoch 26/50, Step: 99/338, total loss:0.0005\n",
      "Epoch 26/50, Step: 149/338, total loss:0.0005\n",
      "Epoch 26/50, Step: 199/338, total loss:0.0005\n",
      "Epoch 26/50, Step: 249/338, total loss:0.0005\n",
      "Epoch 26/50, Step: 299/338, total loss:0.0004\n",
      "Epoch 26, accuracy: 0.9700, validation loss: 0.0188\n",
      "Epoch 27/50, Step: 11/338, total loss:0.0004\n",
      "Epoch 27/50, Step: 61/338, total loss:0.0004\n",
      "Epoch 27/50, Step: 111/338, total loss:0.0004\n",
      "Epoch 27/50, Step: 161/338, total loss:0.0004\n",
      "Epoch 27/50, Step: 211/338, total loss:0.0004\n",
      "Epoch 27/50, Step: 261/338, total loss:0.0004\n",
      "Epoch 27/50, Step: 311/338, total loss:0.0003\n",
      "Epoch 27, accuracy: 0.9700, validation loss: 0.0192\n",
      "Epoch 28/50, Step: 23/338, total loss:0.0003\n",
      "Epoch 28/50, Step: 73/338, total loss:0.0003\n",
      "Epoch 28/50, Step: 123/338, total loss:0.0003\n",
      "Epoch 28/50, Step: 173/338, total loss:0.0003\n",
      "Epoch 28/50, Step: 223/338, total loss:0.0003\n",
      "Epoch 28/50, Step: 273/338, total loss:0.0003\n",
      "Epoch 28/50, Step: 323/338, total loss:0.0003\n",
      "Epoch 28, accuracy: 0.9700, validation loss: 0.0196\n",
      "Epoch 29/50, Step: 35/338, total loss:0.0003\n",
      "Epoch 29/50, Step: 85/338, total loss:0.0003\n",
      "Epoch 29/50, Step: 135/338, total loss:0.0003\n",
      "Epoch 29/50, Step: 185/338, total loss:0.0003\n",
      "Epoch 29/50, Step: 235/338, total loss:0.0002\n",
      "Epoch 29/50, Step: 285/338, total loss:0.0002\n",
      "Epoch 29/50, Step: 335/338, total loss:0.0002\n",
      "Epoch 29, accuracy: 0.9700, validation loss: 0.0199\n",
      "Epoch 30/50, Step: 47/338, total loss:0.0002\n",
      "Epoch 30/50, Step: 97/338, total loss:0.0002\n",
      "Epoch 30/50, Step: 147/338, total loss:0.0002\n",
      "Epoch 30/50, Step: 197/338, total loss:0.0002\n",
      "Epoch 30/50, Step: 247/338, total loss:0.0002\n",
      "Epoch 30/50, Step: 297/338, total loss:0.0002\n",
      "Epoch 30, accuracy: 0.9700, validation loss: 0.0202\n",
      "Epoch 31/50, Step: 9/338, total loss:0.0002\n",
      "Epoch 31/50, Step: 59/338, total loss:0.0002\n",
      "Epoch 31/50, Step: 109/338, total loss:0.0002\n",
      "Epoch 31/50, Step: 159/338, total loss:0.0002\n",
      "Epoch 31/50, Step: 209/338, total loss:0.0002\n",
      "Epoch 31/50, Step: 259/338, total loss:0.0002\n",
      "Epoch 31/50, Step: 309/338, total loss:0.0002\n",
      "Epoch 31, accuracy: 0.9700, validation loss: 0.0206\n",
      "Epoch 32/50, Step: 21/338, total loss:0.0002\n",
      "Epoch 32/50, Step: 71/338, total loss:0.0001\n",
      "Epoch 32/50, Step: 121/338, total loss:0.0001\n",
      "Epoch 32/50, Step: 171/338, total loss:0.0001\n",
      "Epoch 32/50, Step: 221/338, total loss:0.0001\n",
      "Epoch 32/50, Step: 271/338, total loss:0.0001\n",
      "Epoch 32/50, Step: 321/338, total loss:0.0001\n",
      "Epoch 32, accuracy: 0.9700, validation loss: 0.0210\n",
      "Epoch 33/50, Step: 33/338, total loss:0.0001\n",
      "Epoch 33/50, Step: 83/338, total loss:0.0001\n",
      "Epoch 33/50, Step: 133/338, total loss:0.0001\n",
      "Epoch 33/50, Step: 183/338, total loss:0.0001\n",
      "Epoch 33/50, Step: 233/338, total loss:0.0001\n",
      "Epoch 33/50, Step: 283/338, total loss:0.0001\n",
      "Epoch 33/50, Step: 333/338, total loss:0.0001\n",
      "Epoch 33, accuracy: 0.9700, validation loss: 0.0214\n",
      "Epoch 34/50, Step: 45/338, total loss:0.0001\n",
      "Epoch 34/50, Step: 95/338, total loss:0.0001\n",
      "Epoch 34/50, Step: 145/338, total loss:0.0001\n",
      "Epoch 34/50, Step: 195/338, total loss:0.0001\n",
      "Epoch 34/50, Step: 245/338, total loss:0.0001\n",
      "Epoch 34/50, Step: 295/338, total loss:0.0001\n",
      "Epoch 34, accuracy: 0.9700, validation loss: 0.0217\n",
      "Epoch 35/50, Step: 7/338, total loss:0.0001\n",
      "Epoch 35/50, Step: 57/338, total loss:0.0001\n",
      "Epoch 35/50, Step: 107/338, total loss:0.0001\n",
      "Epoch 35/50, Step: 157/338, total loss:0.0001\n",
      "Epoch 35/50, Step: 207/338, total loss:0.0001\n",
      "Epoch 35/50, Step: 257/338, total loss:0.0001\n",
      "Epoch 35/50, Step: 307/338, total loss:0.0001\n",
      "Epoch 35, accuracy: 0.9700, validation loss: 0.0221\n",
      "Epoch 36/50, Step: 19/338, total loss:0.0001\n",
      "Epoch 36/50, Step: 69/338, total loss:0.0001\n",
      "Epoch 36/50, Step: 119/338, total loss:0.0001\n",
      "Epoch 36/50, Step: 169/338, total loss:0.0001\n",
      "Epoch 36/50, Step: 219/338, total loss:0.0001\n",
      "Epoch 36/50, Step: 269/338, total loss:0.0001\n",
      "Epoch 36/50, Step: 319/338, total loss:0.0001\n",
      "Epoch 36, accuracy: 0.9700, validation loss: 0.0224\n",
      "Epoch 37/50, Step: 31/338, total loss:0.0001\n",
      "Epoch 37/50, Step: 81/338, total loss:0.0001\n",
      "Epoch 37/50, Step: 131/338, total loss:0.0001\n",
      "Epoch 37/50, Step: 181/338, total loss:0.0000\n",
      "Epoch 37/50, Step: 231/338, total loss:0.0000\n",
      "Epoch 37/50, Step: 281/338, total loss:0.0000\n",
      "Epoch 37/50, Step: 331/338, total loss:0.0000\n",
      "Epoch 37, accuracy: 0.9700, validation loss: 0.0228\n",
      "Epoch 38/50, Step: 43/338, total loss:0.0000\n",
      "Epoch 38/50, Step: 93/338, total loss:0.0000\n",
      "Epoch 38/50, Step: 143/338, total loss:0.0000\n",
      "Epoch 38/50, Step: 193/338, total loss:0.0000\n",
      "Epoch 38/50, Step: 243/338, total loss:0.0000\n",
      "Epoch 38/50, Step: 293/338, total loss:0.0000\n",
      "Epoch 38, accuracy: 0.9700, validation loss: 0.0232\n",
      "Epoch 39/50, Step: 5/338, total loss:0.0000\n",
      "Epoch 39/50, Step: 55/338, total loss:0.0000\n",
      "Epoch 39/50, Step: 105/338, total loss:0.0000\n",
      "Epoch 39/50, Step: 155/338, total loss:0.0000\n",
      "Epoch 39/50, Step: 205/338, total loss:0.0000\n",
      "Epoch 39/50, Step: 255/338, total loss:0.0000\n",
      "Epoch 39/50, Step: 305/338, total loss:0.0000\n",
      "Epoch 39, accuracy: 0.9700, validation loss: 0.0235\n",
      "Epoch 40/50, Step: 17/338, total loss:0.0000\n",
      "Epoch 40/50, Step: 67/338, total loss:0.0000\n",
      "Epoch 40/50, Step: 117/338, total loss:0.0000\n",
      "Epoch 40/50, Step: 167/338, total loss:0.0000\n",
      "Epoch 40/50, Step: 217/338, total loss:0.0000\n",
      "Epoch 40/50, Step: 267/338, total loss:0.0000\n",
      "Epoch 40/50, Step: 317/338, total loss:0.0000\n",
      "Epoch 40, accuracy: 0.9700, validation loss: 0.0239\n",
      "Epoch 41/50, Step: 29/338, total loss:0.0000\n",
      "Epoch 41/50, Step: 79/338, total loss:0.0000\n",
      "Epoch 41/50, Step: 129/338, total loss:0.0000\n",
      "Epoch 41/50, Step: 179/338, total loss:0.0000\n",
      "Epoch 41/50, Step: 229/338, total loss:0.0000\n",
      "Epoch 41/50, Step: 279/338, total loss:0.0000\n",
      "Epoch 41/50, Step: 329/338, total loss:0.0000\n",
      "Epoch 41, accuracy: 0.9700, validation loss: 0.0242\n",
      "Epoch 42/50, Step: 41/338, total loss:0.0000\n",
      "Epoch 42/50, Step: 91/338, total loss:0.0000\n",
      "Epoch 42/50, Step: 141/338, total loss:0.0000\n",
      "Epoch 42/50, Step: 191/338, total loss:0.0000\n",
      "Epoch 42/50, Step: 241/338, total loss:0.0000\n",
      "Epoch 42/50, Step: 291/338, total loss:0.0000\n",
      "Epoch 42, accuracy: 0.9700, validation loss: 0.0245\n",
      "Epoch 43/50, Step: 3/338, total loss:0.0000\n",
      "Epoch 43/50, Step: 53/338, total loss:0.0000\n",
      "Epoch 43/50, Step: 103/338, total loss:0.0000\n",
      "Epoch 43/50, Step: 153/338, total loss:0.0000\n",
      "Epoch 43/50, Step: 203/338, total loss:0.0000\n",
      "Epoch 43/50, Step: 253/338, total loss:0.0000\n",
      "Epoch 43/50, Step: 303/338, total loss:0.0000\n",
      "Epoch 43, accuracy: 0.9700, validation loss: 0.0249\n",
      "Epoch 44/50, Step: 15/338, total loss:0.0000\n",
      "Epoch 44/50, Step: 65/338, total loss:0.0000\n",
      "Epoch 44/50, Step: 115/338, total loss:0.0000\n",
      "Epoch 44/50, Step: 165/338, total loss:0.0000\n",
      "Epoch 44/50, Step: 215/338, total loss:0.0000\n",
      "Epoch 44/50, Step: 265/338, total loss:0.0000\n",
      "Epoch 44/50, Step: 315/338, total loss:0.0000\n",
      "Epoch 44, accuracy: 0.9700, validation loss: 0.0253\n",
      "Epoch 45/50, Step: 27/338, total loss:0.0000\n",
      "Epoch 45/50, Step: 77/338, total loss:0.0000\n",
      "Epoch 45/50, Step: 127/338, total loss:0.0000\n",
      "Epoch 45/50, Step: 177/338, total loss:0.0000\n",
      "Epoch 45/50, Step: 227/338, total loss:0.0000\n",
      "Epoch 45/50, Step: 277/338, total loss:0.0000\n",
      "Epoch 45/50, Step: 327/338, total loss:0.0000\n",
      "Epoch 45, accuracy: 0.9700, validation loss: 0.0256\n",
      "Epoch 46/50, Step: 39/338, total loss:0.0000\n",
      "Epoch 46/50, Step: 89/338, total loss:0.0000\n",
      "Epoch 46/50, Step: 139/338, total loss:0.0000\n",
      "Epoch 46/50, Step: 189/338, total loss:0.0000\n",
      "Epoch 46/50, Step: 239/338, total loss:0.0000\n",
      "Epoch 46/50, Step: 289/338, total loss:0.0000\n",
      "Epoch 46, accuracy: 0.9700, validation loss: 0.0258\n",
      "Epoch 47/50, Step: 1/338, total loss:0.0000\n",
      "Epoch 47/50, Step: 51/338, total loss:0.0000\n",
      "Epoch 47/50, Step: 101/338, total loss:0.0000\n",
      "Epoch 47/50, Step: 151/338, total loss:0.0000\n",
      "Epoch 47/50, Step: 201/338, total loss:0.0000\n",
      "Epoch 47/50, Step: 251/338, total loss:0.0000\n",
      "Epoch 47/50, Step: 301/338, total loss:0.0000\n",
      "Epoch 47, accuracy: 0.9700, validation loss: 0.0261\n",
      "Epoch 48/50, Step: 13/338, total loss:0.0000\n",
      "Epoch 48/50, Step: 63/338, total loss:0.0000\n",
      "Epoch 48/50, Step: 113/338, total loss:0.0000\n",
      "Epoch 48/50, Step: 163/338, total loss:0.0000\n",
      "Epoch 48/50, Step: 213/338, total loss:0.0000\n",
      "Epoch 48/50, Step: 263/338, total loss:0.0000\n",
      "Epoch 48/50, Step: 313/338, total loss:0.0000\n",
      "Epoch 48, accuracy: 0.9700, validation loss: 0.0438\n",
      "Epoch 49/50, Step: 25/338, total loss:0.0000\n",
      "Epoch 49/50, Step: 75/338, total loss:0.0000\n",
      "Epoch 49/50, Step: 125/338, total loss:0.0000\n",
      "Epoch 49/50, Step: 175/338, total loss:0.0000\n",
      "Epoch 49/50, Step: 225/338, total loss:0.0000\n",
      "Epoch 49/50, Step: 275/338, total loss:0.0000\n",
      "Epoch 49/50, Step: 325/338, total loss:0.0000\n",
      "Epoch 49, accuracy: 0.9667, validation loss: 0.0625\n",
      "Epoch 50/50, Step: 37/338, total loss:0.0000\n",
      "Epoch 50/50, Step: 87/338, total loss:0.0000\n",
      "Epoch 50/50, Step: 137/338, total loss:0.0000\n",
      "Epoch 50/50, Step: 187/338, total loss:0.0000\n",
      "Epoch 50/50, Step: 237/338, total loss:0.0000\n",
      "Epoch 50/50, Step: 287/338, total loss:0.0000\n",
      "Epoch 50/50, Step: 337/338, total loss:0.0000\n",
      "Epoch 50, accuracy: 0.9667, validation loss: 0.0627\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Keywords'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 236\u001b[0m\n\u001b[0;32m    234\u001b[0m test_label \u001b[39m=\u001b[39m [pair[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m results]\n\u001b[0;32m    235\u001b[0m test_data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m test_label\n\u001b[1;32m--> 236\u001b[0m test_data[[\u001b[39m'\u001b[39;49m\u001b[39muuid\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mKeywords\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39msubmit_task2.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\MiniConda\\envs\\pytorch39\\lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32md:\\MiniConda\\envs\\pytorch39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5877\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5879\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5880\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5881\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32md:\\MiniConda\\envs\\pytorch39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5938\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5940\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5941\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Keywords'] not in index\""
     ]
    }
   ],
   "source": [
    "#导入前置依赖\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 用于加载bert模型的分词器\n",
    "from transformers import AutoTokenizer\n",
    "# 用于加载bert模型\n",
    "from transformers import BertModel\n",
    "from pathlib import Path\n",
    "batch_size = 16\n",
    "# 文本的最大长度\n",
    "text_max_length = 128\n",
    "# 总训练的epochs数，我只是随便定义了个数\n",
    "epochs = 50\n",
    "# 学习率\n",
    "lr = 3e-5\n",
    "# 取多少训练集的数据作为验证集\n",
    "validation_ratio = 0.1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 每多少步，打印一次loss\n",
    "log_per_step = 50\n",
    "\n",
    "# 数据集所在位置\n",
    "dataset_dir = Path(\"./data\")\n",
    "os.makedirs(dataset_dir) if not os.path.exists(dataset_dir) else ''\n",
    "\n",
    "# 模型存储路径\n",
    "model_dir = Path(\"./model/bert_checkpoints\")\n",
    "# 如果模型目录不存在，则创建一个\n",
    "os.makedirs(model_dir) if not os.path.exists(model_dir) else ''\n",
    "\n",
    "print(\"Device:\", device)\n",
    "# 读取数据集，进行数据处理\n",
    "\n",
    "pd_train_data = pd.read_csv('./data/train.csv')\n",
    "pd_train_data['title'] = pd_train_data['title'].fillna('')\n",
    "pd_train_data['abstract'] = pd_train_data['abstract'].fillna('')\n",
    "\n",
    "test_data = pd.read_csv('./data/testB.csv')\n",
    "test_data['title'] = test_data['title'].fillna('')\n",
    "test_data['abstract'] = test_data['abstract'].fillna('')\n",
    "pd_train_data['text'] = pd_train_data['title'].fillna('') + ' ' +  pd_train_data['author'].fillna('') + ' ' + pd_train_data['abstract'].fillna('')+ ' ' + pd_train_data['Keywords'].fillna('')\n",
    "test_data['text'] = test_data['title'].fillna('') + ' ' +  test_data['author'].fillna('') + ' ' + test_data['abstract'].fillna('')+ ' ' + pd_train_data['Keywords'].fillna('')\n",
    "\n",
    "# 从训练集中随机采样测试集\n",
    "validation_data = pd_train_data.sample(frac=validation_ratio)\n",
    "train_data = pd_train_data[~pd_train_data.index.isin(validation_data.index)]\n",
    "\n",
    "# 构建Dataset\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mode='train'):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.mode = mode\n",
    "        # 拿到对应的数据\n",
    "        if mode == 'train':\n",
    "            self.dataset = train_data\n",
    "        elif mode == 'validation':\n",
    "            self.dataset = validation_data\n",
    "        elif mode == 'test':\n",
    "            # 如果是测试模式，则返回内容和uuid。拿uuid做target主要是方便后面写入结果。\n",
    "            self.dataset = test_data\n",
    "        else:\n",
    "            raise Exception(\"Unknown mode {}\".format(mode))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 取第index条\n",
    "        data = self.dataset.iloc[index]\n",
    "        # 取其内容\n",
    "        text = data['text']\n",
    "        # 根据状态返回内容\n",
    "        if self.mode == 'test':\n",
    "            # 如果是test，将uuid做为target\n",
    "            label = data['uuid']\n",
    "        else:\n",
    "            label = data['label']\n",
    "        # 返回内容和label\n",
    "        return text, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "train_dataset = MyDataset('train')\n",
    "validation_dataset = MyDataset('validation')\n",
    "train_dataset.__getitem__(0)\n",
    "#获取Bert预训练模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#接着构造我们的Dataloader。\n",
    "#我们需要定义一下collate_fn，在其中完成对句子进行编码、填充、组装batch等动作：\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    将一个batch的文本句子转成tensor，并组成batch。\n",
    "    :param batch: 一个batch的句子，例如: [('推文', target), ('推文', target), ...]\n",
    "    :return: 处理后的结果，例如：\n",
    "             src: {'input_ids': tensor([[ 101, ..., 102, 0, 0, ...], ...]), 'attention_mask': tensor([[1, ..., 1, 0, ...], ...])}\n",
    "             target：[1, 1, 0, ...]\n",
    "    \"\"\"\n",
    "    text, label = zip(*batch)\n",
    "    text, label = list(text), list(label)\n",
    "\n",
    "    # src是要送给bert的，所以不需要特殊处理，直接用tokenizer的结果即可\n",
    "    # padding='max_length' 不够长度的进行填充\n",
    "    # truncation=True 长度过长的进行裁剪\n",
    "    src = tokenizer(text, padding='max_length', max_length=text_max_length, return_tensors='pt', truncation=True)\n",
    "\n",
    "    return src, torch.LongTensor(label)\n",
    "   \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "inputs, targets = next(iter(train_loader))\n",
    "print(\"inputs:\", inputs)\n",
    "print(\"targets:\", targets)\n",
    "\n",
    "#定义预测模型，该模型由bert模型加上最后的预测层组成\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # 加载bert模型\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', mirror='ustc')\n",
    "\n",
    "        # 最后的预测层\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        :param src: 分词后的推文数据\n",
    "        \"\"\"\n",
    "\n",
    "        # 将src直接序列解包传入bert，因为bert和tokenizer是一套的，所以可以这么做。\n",
    "        # 得到encoder的输出，用最前面[CLS]的输出作为最终线性层的输入\n",
    "        outputs = self.bert(**src).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # 使用线性层来做最终的预测\n",
    "        return self.predictor(outputs)\n",
    "\n",
    "#定义模型\n",
    "model = MyModel()\n",
    "model = model.to(device)\n",
    "#定义出损失函数和优化器。这里使用Binary Cross Entropy：\n",
    "criteria = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# 由于inputs是字典类型的，定义一个辅助函数帮助to(device)\n",
    "def to_device(dict_tensors):\n",
    "    result_tensors = {}\n",
    "    for key, value in dict_tensors.items():\n",
    "        result_tensors[key] = value.to(device)\n",
    "    return result_tensors\n",
    "#定义一个验证方法，获取到验证集的精准率和loss\n",
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_correct = 0\n",
    "    for inputs, targets in validation_loader:\n",
    "        inputs, targets = to_device(inputs), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criteria(outputs.view(-1), targets.float())\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        correct_num = (((outputs >= 0.5).float() * 1).flatten() == targets).sum()\n",
    "        total_correct += correct_num\n",
    "\n",
    "    return total_correct / len(validation_dataset), total_loss / len(validation_dataset)\n",
    "\n",
    "# 首先将模型调成训练模式\n",
    "model.train()\n",
    "\n",
    "# 清空一下cuda缓存\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 定义几个变量，帮助打印loss\n",
    "total_loss = 0.\n",
    "# 记录步数\n",
    "step = 0\n",
    "\n",
    "# 记录在验证集上最好的准确率\n",
    "best_accuracy = 0\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # 从batch中拿到训练数据\n",
    "        inputs, targets = to_device(inputs), targets.to(device)\n",
    "        # 传入模型进行前向传递\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失\n",
    "        loss = criteria(outputs.view(-1), targets.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        step += 1\n",
    "\n",
    "        if step % log_per_step == 0:\n",
    "            print(\"Epoch {}/{}, Step: {}/{}, total loss:{:.4f}\".format(epoch+1, epochs, i, len(train_loader), total_loss))\n",
    "            total_loss = 0\n",
    "\n",
    "        del inputs, targets\n",
    "\n",
    "    # 一个epoch后，使用过验证集进行验证\n",
    "    accuracy, validation_loss = validate()\n",
    "    print(\"Epoch {}, accuracy: {:.4f}, validation loss: {:.4f}\".format(epoch+1, accuracy, validation_loss))\n",
    "    torch.save(model, model_dir / f\"model_{epoch}.pt\")\n",
    "\n",
    "    # 保存最好的模型\n",
    "    if accuracy > best_accuracy:\n",
    "        torch.save(model, model_dir / f\"model_best.pt\")\n",
    "        best_accuracy = accuracy\n",
    " \n",
    "#加载最好的模型，然后进行测试集的预测\n",
    "model = torch.load(model_dir / f\"model_best.pt\")\n",
    "model = model.eval()\n",
    "test_dataset = MyDataset('test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "results = []\n",
    "for inputs, ids in test_loader:\n",
    "    outputs = model(inputs.to(device))\n",
    "    outputs = (outputs >= 0.5).int().flatten().tolist()\n",
    "    ids = ids.tolist()\n",
    "    results = results + [(id, result) for result, id in zip(outputs, ids)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = [pair[1] for pair in results]\n",
    "test_data['label'] = test_label\n",
    "test_data['Keywords'] = test_data['title'].fillna('')\n",
    "test_data[['uuid', 'Keywords', 'label']].to_csv('submit_task2.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
