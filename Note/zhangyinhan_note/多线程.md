# 多线程

> 进程：进程可以理解为一个可以独立运行的程序单位
>
> 一个进程可以同时处理很多事情，同时运行互不干扰

为什么能同时做到捅死运行这么多的任务？

**任务对应着线程的执行**



**进程**时线程的集合，是由一个或多个线程构成的

**线程**时操作系统进行运算调度的最小单位，是进程中的最小运行单元

---

> 并发：
>
> 指同一时刻只能由一条指令执行，但是多个线程对应的指令被以非常快的速度快速轮换执行，人根本感觉不到切换，所以宏观上看起来多个线程在同时运行，但是在微观上，是处理器在连续不断地在多个线程之间切换和执行
>
> **在单处理器和多处理器系统中都可以存在仅靠一个核，就可以实现并发**

> 并行：
>
> 有多条指令在多个处理器上同时执行，并行必须要依赖于多个处理器，不论宏观上还是微观上，多个线程都是在同一时刻一起执行的
>
> **只能在多处理器系统中存在，如果只有一个核，就不可能实现并行**

**使用单线程**：处理器必须要等到这些操作完成之后才能继续往下执行其他操作

**使用多线程**：处理器可以在等待时去执行其他的线程，从而整体上提高运行效率



## Thread

计算时间：

```python
start = time.time()
end = time.time()-start
```



### threading模块常用函数：

| 函数                       | 功能-                                 |
| -------------------------- | ------------------------------------- |
| threading.active_count()   | 返回当前处于alive状态的Thread对象     |
| threading.current_thread() | 返回当前Thread对象                    |
| threading.get_indent()     | 返回当前线程的标识符（一个非负整数）  |
| threading.enumerate()      | 返回处于alive状态的所有Thread对象列表 |
| threading.main_thread()    | 返回主线程对象                        |

### Thread对象：

| 成员                            | 说明                         |
| ------------------------------- | ---------------------------- |
| start()                         | 自动调用run方法，启动线程    |
| run()                           | 执行线程                     |
| name                            | 读取线程的名字               |
| is_alive()\ isAlive()           | 线程是否处于alive状态        |
| daemon                          | 布尔值，看线程是否是守护线程 |
| join(timeout=)                  | 设置线程的等待时间或超时返回 |
| _ _init _ _（target,name,args） | 初始化方法                   |

daemon属性：

1.daemon属性为False,主线程结束时会检测子线程是否会结束，子线程未完成，主线程会等待它完成后退出

2.daemon属性为True,主线程结束不会检测子线程是否结束，就自动结束掉





### 线程同步技术：

**程序运行中会发现，多个线程同时运行对同一个变量进行操作，可能某几个线程拿到的值是一样的，所以计算结果是不可预料的。**

所以需要线程同步技术了

> 线程加锁：
>
> 某个线程在对数据进行操作之前，需要加锁，这样其他的线程就不会继续运行，会一直等待锁被释放，只有加锁的线程把锁释放了，其他线程才能继续加锁并对数据进行修改，修改完了再释放锁
>
> 这样可以确保同一时间只有一个线程操作数据，多个线程不会再同时读取和修改同一个数据了，最后的运行结果就是正确的了。



python中由于GIL（Global Interpreter Lock）全局解释器锁的限制，无论是单核还是多核，**在同一时刻只能运行一个进程**。



#### Lock/Locked对象：

一个锁，有两个状态 locked、unlocked . 上锁之后，不属于任何线程

```python
#lock对象的创建
lock = threading.Lock()
lock.acquired()
lock.release()
```

| 对象方法  | 说明                                           |
| --------- | ---------------------------------------------- |
| acquire() | locked和unlocked互相切换                       |
| release() | 只能locked修改成unlocked，反过来回来就抛出异常 |

#### Condition对象：

condition对象可以在某些事件触发后才处理数据，可以用于不同线程之间的通信或通知

| 对象方法     | 说明 |
| ------------ | ---- |
| wait()       |      |
| notify()     |      |
| notify_all() |      |

#### Queue对象：

队列的形式进行信息交换，尤其适合多个线程之间进行信息交换

myqueue = queue.Queue()



#### Event对象：

一个线程设置Event对象，另一个线程等待Event对象

| 对象方法 |                   说明                   |
| -------- | :--------------------------------------: |
| set()    |        设置Event对象内部信号为真         |
| isSet()  |          判断对象内部的信号标志          |
| clear()  |          清除对象内部的信号标志          |
| wait()   |          内部信号为真，很快执行          |
|          | 内部信号为假，一直等待至超时或信号变成真 |



### 多线程案例

#### 使用线程池爬取妹子图

```python
# encoding = utf-8
import concurrent
import os
from concurrent.futures import ThreadPoolExecutor
import requests
from bs4 import BeautifulSoup


def header(referer):

    headers = {
        'Host': 'i.meizitu.net',
        'Pragma': 'no-cache',
        'Accept-Encoding': 'gzip, deflate',
        'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
        'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
        'Referer': '{}'.format(referer),
    }

    return headers


def request_page(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            return response.text
    except requests.RequestException:
        return None


def get_page_urls():

    for i in range(1, 2):
        baseurl = 'https://www.mzitu.com/page/{}'.format(i)
        html = request_page(baseurl)
        soup = BeautifulSoup(html, 'lxml')
        list = soup.find(class_='postlist').find_all('li')
        urls = []
        for item in list:
            url = item.find('span').find('a').get('href')
            print('页面链接：%s' % url)
            urls.append(url)

    return urls


def download_Pic(title, image_list):
    # 新建文件夹
    os.mkdir(title)
    j = 1
    # 下载图片
    for item in image_list:
        filename = '%s/%s.jpg' % (title, str(j))
        print('downloading....%s : NO.%s' % (title, str(j)))
        with open(filename, 'wb') as f:
            img = requests.get(item, headers=header(item)).content
            f.write(img)
        j += 1

def download(url):
    html = request_page(url)
    soup = BeautifulSoup(html, 'lxml')
    total = soup.find(class_='pagenavi').find_all('a')[-2].find('span').string
    title = soup.find('h2').string
    image_list = []

    for i in range(int(total)):
        html = request_page(url + '/%s' % (i + 1))
        soup = BeautifulSoup(html, 'lxml')
        img_url = soup.find('img').get('src')
        image_list.append(img_url)

    download_Pic(title, image_list)


def download_all_images(list_page_urls):
    # 获取每一个详情妹纸
    # works = len(list_page_urls)
    
    #设置线程池，最大线程为5
    with concurrent.futures.ProcessPoolExecutor(max_workers=5) as exector:
        for url in list_page_urls:
            exector.submit(download, url)


if __name__ == '__main__':
    # 获取每一页的链接和名称
    list_page_urls = get_page_urls()
    download_all_images(list_page_urls)
```



#### 多线程豆瓣

```python
def main(url):
    html = request_douban(url)
    soup = BeautifulSoup(html, 'lxml')
    save_content(soup)

if __name__ == '__main__':
    start = time.time()
    urls = []
    pool = multiprocessing.Pool(multiprocessing.cpu_count())
    #cpu_count是根据电脑的cpu数量，来计算线程池中线程数
    
    for i in range(0, 10):
        url = 'https://movie.douban.com/top250?start=' + str(i * 25) + '&filter='
        urls.append(url)
        
    pool.map(main, urls)
    #把urls传入到main函数中
    pool.close()
    #close方法是不再创建进程，不是关闭的意思
    pool.join()
    #join方法是让进程执行完毕再结束
```

#### 爬图

```python
import requests
from lxml import etree
from urllib import request
import re
import os
import threading
from queue import Queue
import time

# 技术点
# 1.使用request是获取html数据
# 2.使用xpath解析数据
# 3.使用正则表达式sub()函数过滤掉特殊的字符
# 4.使用urllib.request.urlretrieve()下载图片
# 5.生产者和消费者模式分离
# 6.使用queue[线程安全]去保存【每一页的爬取地址】和【表情图片地址】

HEADERS = {
	'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'
}


class Procuder(threading.Thread):
	"""
	生产者
	爬取页面，获取图片地址加入到表情图片队列中
	"""

	def __init__(self, name, page_queue, img_queue, *args, **kwargs):
		super(Procuder, self).__init__(*args, **kwargs)
		self.name = name
		self.page_queue = page_queue
		self.img_queue = img_queue

	def run(self):
		while True:
			if self.page_queue.empty():
				print(self.name + '任务完成~')
				break
			# 1.获取每一页的url
			page_url = self.page_queue.get()

			# 2.爬取页面的数据
			self.spider_page(page_url)

			# 3.休眠0.5秒
			time.sleep(0.5)

	def spider_page(self, url):
		"""
		爬取每一页
		:param url: 每一页的地址
		:return:
		"""
		response = requests.get(url, headers=HEADERS)
		text_raw = response.text

		# 1.使用etree
		html_raw = etree.HTML(text_raw)

		# 2.使用xpath解析数据
		# 注意：过滤掉gif标签图片
		imgs = html_raw.xpath('//div[@class="page-content text-center"]//img[@class!="gif"]')

		# 3.获取图片的实际连接并下载到本地
		for img in imgs:
			# 3.1 图片的实际地址
			img_url = img.get('data-original')

			# 3.2 图片名称替换特殊符号
			alt = re.sub(r'[\?？\.，。！!\*]', '', img.get('alt'))

			# 3.3 提取图片的后缀,组装成文件的名字
			img_name = alt + os.path.splitext(img_url)[-1]

			# 3.4 把爬取到的表情【图片地址+图片名称】以【元组】的形式加入到队列图片队列中
			self.img_queue.put((img_url, img_name))


class Consumer(threading.Thread):
	"""
	消费者
	获取图片的地址下载到本地
	"""

	def __init__(self, name, page_queue, img_queue, *args, **kwargs):
		super(Consumer, self).__init__(*args, **kwargs)
		self.name = name
		self.page_queue = page_queue
		self.img_queue = img_queue

	def run(self):
		while True:

			if self.img_queue.empty() and self.page_queue.empty():
				print(self.name + '任务完成~')
				break

			# 1.解包，获取图片的地址 + 图片的名称
			img_url, img_name = self.img_queue.get()

			# 2.使用urlretrieve()函数下载图片到本地
			request.urlretrieve(img_url, './imgs/%s' % img_name)

			print(img_name + "下载完成")


def spider():
	# 1.页面的队列
	page_queue = Queue(100)

	# 2.表情图片的队列
	# 注意：队列的大小尽量设置大一些，保证线程减少等待的时间
	img_queue = Queue(1000)

	# 3.爬取页面的地址
	for x in range(1, 10):
		url = 'http://www.doutula.com/photo/list/?page=%d' % x

		# 3.1 存入到页面地址队列中
		page_queue.put(url)

	# 创建5个生成者和5个消费者
	# 生产者：爬取每一页的数据，获取表情图片的url
	# 消费者：从表情队列中获取表情图片的实际地址并下载到本地
	for x in range(5):
		t = Procuder(name='生产线程-%d' % x, page_queue=page_queue, img_queue=img_queue)
		t.start()

	for x in range(5):
		t = Consumer(name='消费线程-%d' % x, page_queue=page_queue, img_queue=img_queue)
		t.start()


if __name__ == '__main__':
	spider()
```



# 多进程

由于进程中的GIL的存在，python使用多线程并不能大幅度提高任务的吞吐量和处理速度。

但是对于多进程来说，每个进程都有属于之间的GIL，所以在多核处理下，多进程的运行不会受GIL的影响

进程间相互独立，所以各个进程之间的数据无法共享的，需要有单独的机制



multiprocessing 

| multiprocessing的类 | 说明   |
| ------------------- | ------ |
| Process             | 进程   |
| Queue               | 队列   |
| Semaphore           | 信号量 |
| Pipe                | 管道   |
| Lock                | 锁     |
| Pool                | 进程池 |

## Process

### 参数：

Process(group,target,name,args/kwargs)

* target 表示调用对象，传入调用的函数

* args表示被调用对象的位置参数元组，必须是元组，单个元素加逗号（i, ）

  比如target函数是func，他有两个参数m,n 那么args就传入[m,n]

* kwargs表示调用对象的字典

* name是别名，相当于给这个经常取一个名字

* group是分组

### 类方法

| 成员                            | 说明                         |
| ------------------------------- | ---------------------------- |
| start()                         | 自动调用run方法，启动线程    |
| run()                           | 执行线程                     |
| name                            | 读取线程的名字               |
| is_alive()                      | 线程是否处于alive状态        |
| daemon                          | 布尔值，看线程是否是守护线程 |
| join(timeout=)                  | 设置线程的等待时间或超时返回 |
| _ _init _ _（target,name,args） | 初始化方法                   |
| terminate                       | 杀死进程                     |

多个进程同时输出，输出可能会乱掉，因为并行的缘故。

```python
from multiprocessing import Process

def f(name):
    print('hello', name)

if __name__ == '__main__':
    p = Process(target=f, args=('xiaoshuaib',))
    p.start()
    p.join()
```



### 进程互斥锁：

lock = Lock()

我们要让多个进程运行期间任一时期只能一个进程输出，其他进程等待之前哪个进程输出之后再输出，这样就不会乱了。

## 信号量 Semaphore

> 信号量：可以控制临界资源的数量，实现多个进程同时访问共享资源，限制进程的并发量



```python
sema = Semaphore(3)#同一时刻最多允许三个线程访问资源
for i in range(10):
    t = Thread(target=worker,args=(i,))
    t.start()
```



### 队列Queue：

> 让进程共享数据

```python
import multiprocessing as mp
mp.set_start_method('spawn') #windows系统创建子进程的默认方式
q = mp.Queue()
p=mp.Process(target=foo,args=(q,))
p.start()
p.join()
```



### 管道Pipe

> 可以理解为两个进程之间通信的通道
>
> - **单向管道**，即half-duplex
>
> 一个进程负责发消息，一个进程负责收消息
>
> - **双向管道**，即duplex
>
> 互相收发消息

```python
from multiprocessing import Process,Pipe
parent_conn , child_conn = Pipe()
p = Process(target=foo,args=(i,))
p.start()
```





### 进程池Pool

> 可以提供指定数量的进程，供用户调用
>
> 当有新的请求提交到pool中时，如果池还没有满，就会创建一个新的进程用来执行该请求
>
> 如果池中的进程数已达到规定的最大值
>
> 那么该请求就会等待，直到池中有进程结束，才会创建新的进程来执行它

```python
from multiprocessing import Pool
import time

def function(index):
    print(f'Start process :{index}')
    time.sleep()
    print(f'End process {index}')
    
pool= Pool(processes=3)
for i in range(4):
    pool.apply_async(function,args=(i,))
    print('Main Process started')
    pool.close()
    pool.join()
    print('Main Process Ended')
```

map方法：map(function,可迭代对象)

![1635858367496](C:%5CUsers%5C%E9%93%B6%E6%99%97%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1635858367496.png)

```python
with Pool(5) as p:
    print(p.map(f(函数),x(可迭代对象)))
```

